{
  "url": "https://garnix.io/blog/garn2/",
  "summary": {
    "summary": "文章讨论构建系统中常被忽视的“求值阶段缓存”：不仅缓存构建产物，也缓存“决定要构建什么”的计算结果，尤其针对 Nix 在大仓库/CI 中求值耗时过长的问题。作者以自研的 Nix 替代前端 garn（TypeScript 作为前端语言）为例，通过将求值限制为纯/确定性执行并记录真实读取依赖，实现细粒度的求值缓存。基准测试显示，在无关文件变更或无变更时，garn 的求值可从数百毫秒/秒级降到十几毫秒，并且缓存不会因仓库任意变更而整体失效。",
    "core_value": "用“基于真实依赖的前向求值缓存”显著降低构建系统的求值开销，避免像 Nix flakes 那样因仓库状态变化导致缓存粗粒度失效。直接改善本地开发体验（如进入 devshell）与 CI 时延，尤其适用于大规模单仓场景。",
    "tech_stack": [
      "Nix",
      "TypeScript",
      "V8",
      "Deno",
      "Rust",
      "rustyscript",
      "内容哈希/可复现构建",
      "锁文件(Garn.lock)",
      "依赖追踪(文件读取/目录快照)"
    ],
    "recommendation": "把“求值”当作可缓存的纯函数来设计，并用运行时依赖追踪生成更精确的缓存键，这个思路对 Nix 及其他声明式/脚本化构建系统都有借鉴意义。文中给出可对比的基准数据与工程化约束（限制副作用、锁文件、内容寻址目录快照），可直接启发构建性能优化实践。",
    "score": 90.0,
    "_source_item": {
      "title": "Forwardly-evaluated build systems",
      "content": "Giulio Camillo Delminio's Theater of Memory, an imagined structure meant to serve as the ideal memory palace.\nBuild tools are very attentive to opportunities of caching build artifacts —\nthat's a large part of their job.\nOne area that is neglected caching of the\nevaluation\nneeded to figure out what to build\n— even though in some of these systems that phase can be very slow!\nIn Nix in particular, large monorepos can see evaluation times\nin the minutes\n,\nwhich substantially increases CI times. At the other end of the spectrum, smaller\nprojects too soon hit UX difficulties with Nix as evaluation is on the critical\npath for commands like entering a devshell. There, shaving off even a couple\ndozen milliseconds matters.\nIn our own work on an alternative Nix frontend, we looked at various\ntechniques for speeding up evaluation by caching. The results are very\npromising, and the techniques applicable in other projects.\nThe Nix pipeline is roughly this:\nA Nix expression is evaluated; the result should be a derivation datatype. Those\nare serialized into derivation files, which are the normalized build recipes.\nThe builder then builds these and puts the resulting artifacts in the Nix store.\nIn reality the story is more complicated. There are in fact arrows from the\nderivation files\nback\nto evaluation.\nThese occur most notably when a\nderivation datatype in Nix is converted into string, as that string is the\npath of the derivation file it produces, and rather than duplicate work later\non, Nix creates the derivation files\nduring\nevaluation. A second arrow, from\nbuilt artifacts back to Nix expressions, is what's known as\nimport-from-derivation (IFD), and allows Nix expressions to require that something\nbe built\nduring\nevaluation (if that thing hasn't been built yet), and to then\nread files that were built.\nThe derivation files constitute the interface between Nix-the-language and\nNix-the-builder. Given this API, there is no reason why a different surface\nlanguage couldn't be used.\nA while back, we decided to try to make TypeScript the frontend language. It's\nfamiliar to a lot more people, and moreover, Nix has a lot of problems that\ncome from being relatively obscure (it's slower, has less nice error messages,\ndocumentation generation is bad, LSPs aren't as advanced, etc.). The result\nwas\ngarn\n.\nOur first version looked like this:\nIt created Nix strings in TypeScript, and then passed it on to the usual\npipeline. The experience of\nwriting\ngarn files was great, but because of\nthis extra step, everything was slower.\nThat's quite unfortunate, because slow\nNix\nevaluation is already a problem.\nTo fix this, the natural thing to do is to skip Nix evaluation entirely. TypeScript\nis a faster language than Nix, so that should be a good basis to start. We created\na new TypeScript runtime that takes the V8 sandbox (via the great Deno/rustyscript\nlibraries), removes almost all the primops that are side-effects (writing files,\ndoing network fetches, random numbers), and put in just:\nreadFile(file: string)\n. Reads a file *from within the same directory\nas  the\ngarn.ts`/entrypoint file.\npath(path: string, { gitignore?: bool })\n. Given a directory, adds\n(potentially filtering) all files of the directory to a read-only copy, and\nreturns the path of that copy (which is named by the hash of the contents).\nimport(URL/file/git-repo: string)\n. Read a file or git repo from a URL or\nlocal file path. If it's a remote file, add its hash to\nGarn.lock\n.\nIn other words, we made TypeScript be a pure and deterministic language if\nGarn.lock\nand the entire directory is considered as input. In fact, it is\ndeterministic as a function of Garn.lock and exactly the files it reads during\nexecution. (This observation, we'll see, is crucial: it means we can make the\ncache key depend on actual dependencies, rather than the entire repository.)\nWe've been calling this version\ngarn2\n,\nthough throughout this blog I'll refer to it as simply “garn” when that is not\nambiguous.\nUsing TypeScript seems to have paid off — our baseline performance is\nquite good.\nBenchmark 1: garn.ts (change Cargo.toml)\nTime (mean ± σ):     455.1 ms ±  35.1 ms    [User: 292.5 ms, System: 90.9 ms]\nRange (min … max):   413.4 ms … 530.5 ms    10 runs\n\nBenchmark 2: default.nix (change Cargo.toml)\nTime (mean ± σ):     877.1 ms ±  25.8 ms    [User: 571.8 ms, System: 102.1 ms]\nRange (min … max):   836.0 ms … 928.3 ms    10 runs\n\nBenchmark 3: flake.nix (change Cargo.toml)\nTime (mean ± σ):      1.603 s ±  0.205 s    [User: 0.721 s, System: 0.127 s]\nRange (min … max):    1.420 s …  1.972 s    10 runs\n(A note on methodology: for each type of benchmark, we run the build first, so\nlibraries etc. are imported. Then we change a source file, and time the build\nagain (doing this 10 times each). We'll come back to why there's such a big\ndifference between\ndefault.nix\nand\nflake.nix\n.)\n?\nThe story of getting to that baseline is for another time. In this blog\npost we're going to improve that, specifically with caching.\nNix, when used with flakes, has a form of evaluation caching. It stores maps\nbetween repo state and flake output attributes to value; so if you evaluated\npackages.x86_64-linux.foo\nand it returns\n/nix/store/...-foo.drv\n, that\nmapping will be cached. If you again evaluated that same attribute, Nix won't\nhave to compute it again.\nThis saves time, but only in the very narrow case where it applies. Most\nnotably, if\nanything\nchanges in the repository, all the caches are\ninvalidated.\n?\nEven if the change is to something completely unrelated to your evaluation, it\nwill invalidate the cache.\nThis is annoying locally, but in CI, and especially for monorepos, this is a big\nmissed opportunity.\nInstead, what garn does is, on evaluation, keep a record of the hash\nof\ngarn.ts\n,\nand\nof all reads that the evaluation did. If you imported\nsomething, or called\npath\nor\nreadFile\n, that's tracked. The result is also\ncached at the end.\nOn subsequent runs, garn checks whether it has a cache for that version of\ngarn.ts\n. If it does, it checks what reads the last run did, and whether the\nresults are all the same now. If so, it returns the cached value.\n?\nIt's safe to do this because garn scripts are as pure as a\nfunction of those inputs.\nBenchmark 1: garn.ts (change nothing)\nTime (mean ± σ):      16.8 ms ±   3.0 ms    [User: 4.4 ms, System: 12.5 ms]\nRange (min … max):     9.9 ms …  22.7 ms    39 runs\n\nBenchmark 2: default.nix (change nothing)\nTime (mean ± σ):     865.7 ms ±  31.5 ms    [User: 581.1 ms, System: 100.4 ms]\nRange (min … max):   812.8 ms … 914.5 ms    10 runs\n\nBenchmark 3: flake.nix (change nothing)\nTime (mean ± σ):     213.2 ms ±   4.2 ms    [User: 54.6 ms, System: 24.0 ms]\nRange (min … max):   206.6 ms … 219.2 ms    10 runs\ngarn takes about\n2%\nof the time\ndefault.nix\ndoes, and 7% of the (also cached)\nflake.nix\n!\nThe above is a comparison of evaluating when\nnothing\nhas changed. But for garn,\nthe cache is only invalidated when it should be\n. If you have a\npackage that only reads some directories or files, and imports only certain\nmodules, it will retain its evaluation cache if only\nother\nthings change.\nBenchmark 1: garn.ts (change README.md)\nTime (mean ± σ):      15.8 ms ±   1.4 ms    [User: 4.3 ms, System: 11.8 ms]\nRange (min … max):    11.4 ms …  18.4 ms    44 runs\n\nBenchmark 2: default.nix (change README.md)\nTime (mean ± σ):     808.9 ms ±  30.5 ms    [User: 572.8 ms, System: 103.2 ms]\nRange (min … max):   753.2 ms … 849.0 ms    10 runs\n\nBenchmark 3: flake.nix (change README.md)\nTime (mean ± σ):      1.112 s ±  0.026 s    [User: 0.728 s, System: 0.119 s]\nRange (min … max):    1.089 s …  1.171 s    10 runs\n(In the repo we are benchmarking,\nflatbuffers\n, 97% of commits don't touch the Rust code. The\nflake.nix\ncache is not\nhelpful for 100% of commits; the garn cache is valid for 97% of them. The\npercentage won't always be this high, of course.)\nWhen you have large projects where evaluation alone is\nhuge, that's a big deal, not just in terms of time, but also in terms of the\nmemory usage you can save.\nThis cache avoids even entering the V8 runtime entirely. But there's a downside\nto it: if certain computations that are expensive are used by two different\noutputs (e.g.,\ngarn build foo\nand\ngarn build bar\nboth compute some\nbaz\n),\nthe cache won't be reused between them. (The Nix flake cache also suffers from\nthis.) We'll return to this.\nThis isn't a new idea.\nfabricate\n,\nmemoize\n,\nrattle\nand others have based build systems\non tracing effects.\nSpall et al (2022)\ncall these build systems\nforward\nbuild systems\n; unlike\nMake\nand the like, they don't reason backwards from\na target, satisfying its dependencies, but instead run the build script forwards\nonce, keeping track of its dependencies. Most of them use\nstrace\nor the like\n(on Linux) to observe these dependencies or side effects. The ideal is that\nyou can run\nan unmodified program\n(e.g. a bash script) and get an\nautomatically cachable version of it. The possible side effects in an arbitrary\nLinux program are, however, huge, and keeping track of them efficiently and\ncorrectly is quite difficult. This is even harder to do in a cross-platform way.\nAs a result, these programs fall somewhat short of their ideal. With garn, on\nthe other hand, we keep track of side-effects at a language level, which is\nmuch easier to do and do correctly.\nThese programs are also about forward\nbuilds\n; what garn is doing is using\ntracing at\nevaluation time\n, to figure out what the build recipe is.\nThere's a pattern that a lot of big Nix repos use to bring down their\nevaluation time that would be abominable if it weren't necessary. It's moving\nevaluation into IFDs so that the evaluation is cached\nas a build artifact\n.\nThere are a few ways of doing this, but one is to check the output of\nnix eval <expensive computation>\nto your git repo, regenerating as needed. Then\nyou have your Nix code import that.\nThe problem isn't only that keeping generated code in your repo, and keeping it\nup-to-date, is a pain. It's also that the existence of IFD itself negatively\naffects the performance of evaluation. Suddenly you have to spin up a whole\nsandbox, and potentially install in it a lot of software, to calculate some\nvalue. This is made worse by the fact that there is no concurrency and very\nlittle parallelism in Nix (in evaluation, at any rate). Moreover, because Nix\nbuilds are only “best-effort” deterministic (not doing anything about\nnon-determinism introduced by parallelism, for example), IFD degrades the\ndeterminism of\nevaluation\nto that otherwise-lower level one of builds.\nBut there is an important idea there. It's to split the expensive computation\ninto parts, so that one part depends on as little as possible, so as to get\na form of incrementalism. After all, even if our previous cache was\nprecise\n,\nin that it didn't invalidate a computation unless it really had to, it was\nall-or-nothing: either the\nentire\ncomputation was cached or none of it.\nOur approach to supporting a form of user-guided incrementalism was to\nprovide language-level access to something very similar to the cache mentioned\nin the previous section. The API is via a function\nmemoize\n, that takes two\narguments: a\nfile\nwith a\nmain\nfunction to be memoized, and the arguments to\ncall it with. The file can have imports, or read files, just like everything\nelse. (It can even\nitself\ncall\nmemoize\n!). Additionally, it can call\ngetArgs\nto get its arguments. These effects are tracked as above. On rerun,\nif its reads didn't change, the result can be taken from the cache. Because it\nis a separate file, it is easy to prevent mutable variables from being shared\nbetween the computation that's memoized and the world outside (which would\ncompromise cache correctness). (This is in fact the same reasoning that led\nWeb Workers to being designed the way they did, as a separate script. The\nmotivation there relates to parallelism rather than memoization, but the\nsolution is the same. Indeed, our\nmemoize\nuses\nweb workers, which has the\nadded advantage of providing more opportunities for parallelism.)\nThis approach of tracking\ngetArgs\nas an\neffect\n, we realized, makes it\npossible to solve the problem we encountered earlier, of the cache not being\nshared between different computations. The original design of garn makes the\navailable targets be whatever derivations/packages you happen to export; garn\ndoes the job of matching the name you provided in the command line (e.g.,\nfoo\nin\ngarn build foo\n) with a variable your\ngarn.ts\nexported. But we could\ninstead provide a\ngetTarget\nfunction which returns the target\nand track its\nusage as an effect\n. Any calls to\nmemoize\nthat happen\nbefore\ngetTarget\nwon't be invalidated (if none of the other reads changed).\nThe results:\nBenchmark 1: garn.ts (change rust/flatbuffers/src/lib.rs)\nTime (mean ± σ):     311.5 ms ±  14.2 ms    [User: 227.6 ms, System: 76.3 ms]\nRange (min … max):   283.9 ms … 330.7 ms    10 runs\n\nBenchmark 2: default.nix (change rust/flatbuffers/src/lib.rs)\nTime (mean ± σ):     866.4 ms ±  20.2 ms    [User: 570.0 ms, System: 97.2 ms]\nRange (min … max):   836.4 ms … 892.5 ms    10 runs\n\nBenchmark 3: flake.nix (change rust/flatbuffers/src/lib.rs)\nTime (mean ± σ):      1.552 s ±  0.173 s    [User: 0.715 s, System: 0.128 s]\nRange (min … max):    1.402 s …  2.006 s    10 runs\nAbout 70% of the time of the uncached version (which was, as a reminder, 455ms),\nand nearly one fifth of\nflake.nix\n. Not bad, but not as fast as the\nfully-cached computation, which was in fact 10x faster.\nThis is as far as we've implemented caches. But let's consider what could be\nimproved next.\nOne caching improvement we won't talk about much is adding a post-processing\nfunction to many of the side-effect primops. The idea is that the function can\nselect a subset of the data in e.g. a file that it cares about (for instance,\neverything but the comments and spacing in\nCargo.toml\n. The whole primop would\nthen return that value instead. The point is that cache invalidation could\nthen only occur if that\nprocessed\nvalue changes.\nFor certain stacks this can be helpful, but not the ones we've been looking at.\nThere are few reasons for this difference:\nYou have to spin up the whole V8 VM\nsince there is stuff to do before and\nafter getting the memoized value.\nYou actually have to do (i.e., compute) that stuff\nWhat's in the cache could be quite big, and storing and retrieving that takes more time\nYou then have to pass a relatively big datastructure from TypeScript into\nRust, and let it be processed.\nIf you're trying to get the eval down from a few\nminutes\nto a few\nseconds or a few hundred milliseconds, that might not be relevant; but if you\nalso care about going down to\ntens of milliseconds\n, it matters a lot.\nIf you look at most stacks, they follow a common pattern: there are a couple of\nfiles (\nCargo.toml\n,\nCargo.lock\n,\npackage.json\n,\npackage-lock.json\n,\n<project>.cabal\n, etc.) which determine\nalmost everything\nabout the\nresulting derivation — i.e., of the build —\nexcept\nwhat source files\nare in the directory when the build kicks off. Almost all the work that happens\nduring the evaluation\ndoesn't need to know about the actual source. It also\nhappens that those config and lock files change much more rarely than source\nfiles themselves.\nIn essence, we have something like\nbuildPackage(configFiles, src) -> Derivation\n,\nbut it's possible to split it into\nbuildPackageBuilder(configFiles) -> (src -> Derivation)\nsuch that all the expensive work happens by the time the first function returns.\nIf we could in this way partially evaluate\nbuildPackage\nand then cache the\nresult\n, we would have evaluations that are essentially instantaneous\nunless a config file changes\n. Caching e.g. a parsed config file might only be\npart of the computation that's independent of\nsrc\n, and it's not always easy\nto refactor things to include more of it. But if we could return the partially\nevaluated function, things would be much easier.\nFunctions aren't serializable, at least not easily; since the cache is persistent,\nthat means we can't cache the function itself.\nThe “common” case is that we return a datatype (the derivation) that doesn't vary\nstructurally based on the source, but only in the values of certain primitive\nfields (mostly strings/paths). So when user code returns a value that has a source\npath, we can figure out where in that value the source is used (by diffing two\nversions of it, by just inspecting the string, or by more reasonable but\ninvolved techniques such as keeping metadata of usage, sort of like “taints”).\nWe can then in the Rust side do as much processing as doesn't touch that value\n(adding dependency derivations and calculating hashes), and then store just the\nremainder, with a pointer to what needs to be changed. We even control the\nlength of the string, so it might even be possible to operate directly on the\nbytes of the persisted data.\nThe problem we had is that calls to\npath\ninvalidate the cache, even though\nusually the path is just a\nbuild-time\ndependency (in the sense that\nthe derivations generally produced when paths differ differ\nonly\nby substitution\nof one path by the other; there's no branching or parsing of the store path\ngoing on). We came up with a technique for very fast caching in case that\nassumption of how\npath\nis used is true. To make this safe, the type returned\nby\npath\nmust be opaque so it can never be inspected.\nThe upside of all this is a fast cache that does not get invalidated by\ncalls to\npath\n, and which requires no change from the user's side.\nAll of that is quite a bit of work, however. So far we've been treating this\nversion of garn as more of an exploration; most of these caches took only\na couple of days to implement. Likely we won't continue in this direction for\nnow, but it's good to know the option exists.\nHere are all the results gathered together:\nNot bad.\nThough this version of garn can already build itself, it is by no means ready\nfor production, and not only because of missing features. It's an experimental\nimplementation, about exploring ideas more than being production-grade. (It\nalso involved a fair bit of LLM code.) The code itself can be found\nhere\n.\nBesides caching, there are lots of ways performance can be (or has been)\nimproved. A lot of these are also really exciting.\nIn theory a lot of these improvements could be ported to Nix as well:\nNix-the-language is already deterministic and has a way to control access to mutable\nvariables (it disallows them altogether), which are the main requirements. In\npractice I suspect they would require too much change in the codebase to be\nrealistic.\nIf you have open source repositories where evaluation is in the minutes, reach\nout to us! Having these examples is great for guiding performance improvements.\nSimilarly, if you would like to collaborate on garn in some capacity, also\nreach out, on\nMatrix\n,\nDiscord\n, or by\nemail\n.",
      "description": "",
      "id": null,
      "short_id": null,
      "url": "https://garnix.io/blog/garn2/",
      "score": "29",
      "tags": "nix",
      "domain": "garnix.io",
      "author": "jkarni",
      "author_url": "jkarni",
      "time": "17 hours ago",
      "comments_text": "7 comments",
      "comments_url": "7 comments",
      "description_hint": null,
      "source": "Lobsters"
    }
  },
  "cached_at": "2026-02-12T05:06:13.757181"
}
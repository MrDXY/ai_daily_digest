{
  "url": "https://www.testingbranch.com/re_identification/",
  "payload": {
    "title": "Re-Identification Risk vs k-Anonymity",
    "content": "Code:\ngithub.com/mpcsb/reidentification\nRe-Identification Risk vs k-Anonymity: An Experimental Walkthrough\nMost discussions of anonymization focus on buzzwords like\nk-anonymity\nand\ndifferential privacy\n, but few dig into what actually happens to a dataset as anonymity strength increases.\nIn this post, we conduct a full experimental walkthrough to quantify how raising the k-anonymity level impacts both\nprivacy\n(re-identification risk) and\ndata utility\n.\nWe simulate an attacker with partial knowledge trying to re-identify individuals, and we measure how data quality degrades as we ramp up the anonymization.\nThe goal is to illuminate where the balance lies between keeping data useful and keeping individuals anonymous.\nData Generation and Anonymization Setup\nFor our experiment, we generated a synthetic dataset of\n2000 individuals\n, each with the following fields:\nField\nDescription\nage\nNumerical age (used as a quasi-identifier)\nzip3\n3-digit ZIP code prefix (regional location, quasi-identifier)\nsex\nBinary sex attribute (quasi-identifier)\nlab_glucose\nContinuous lab glucose level (a target variable\nnot\nused in anonymization)\nWe treat\nage\n,\nzip3\n, and\nsex\nas the quasi-identifiers (QIs) that will be subject to anonymization.\nThe value of\nk\nin k-anonymity was varied from\n1 to 20\n.\nA k-anonymity requirement means each record must be indistinguishable from at least\nk–1 others\nwith respect to these QIs.\nTo achieve this, an anonymization routine\ngroups and generalizes\nrecords until every combination of QIs occurs in at least k records.\nIn practical terms, as k increases, the algorithm must increasingly\ngeneralize (coarsen)\nor\nsuppress\ndetails in the QIs to satisfy the larger group size.\nParameters explored\nWe explored a few key parameters that control how the data is generalized:\nAge bin width:\nWe varied age grouping from 1-year bins (no grouping beyond integer ages) up to 10-year bins. Larger bin widths mean ages get lumped into broader ranges (e.g., 30–39).\nTop-coding of age:\nExtreme ages were top-coded above a threshold (e.g., all ages 75 and above recorded as\n\"75+\"\n). This prevents very old ages from standing out.\nRare ZIP suppression:\nLow-frequency ZIP3 regions were grouped into an\n\"Other\"\ncategory once their count fell below a threshold. If a region is too unique, it gets collapsed to hide outliers.\nBy adjusting these knobs, we impose different anonymization strategies.\nFor each value of\nk\n(and each combination of binning/top-coding settings), we produced an anonymized version of the dataset and evaluated how much information was lost in the process.\nAttacker Context: Partial Knowledge Threat Model\nAnonymization is only meaningful relative to an attacker’s knowledge.\nIn our scenario, we simulate an attacker who has\npartial information\nabout individuals — specifically, the attacker knows an individual’s:\nage\ngeneral location (ZIP3 region)\nsex\n(e.g., from a data leak or public records).\nThis is a common threat model for re-identification:\nan adversary might obtain someone’s demographic details from a breached source and then try to find that person in an anonymized dataset (such as medical or survey data) released publicly.\nThe attacker’s goal is\nre-identification\n:\nto match each anonymized record to the corresponding real individual by comparing the quasi-identifiers.\nImportantly, the attacker does\nnot\nknow the sensitive value (\nlab_glucose\n) in our case;\nthey only leverage the QIs that are also present (albeit generalized) in the anonymized data.\nThis kind of attack is known as a\nrecord linkage attack\n, using the assumption that if an anonymized entry shares a unique combination of age, region, and sex with a known individual’s data, they are likely the same person.\nThis threat model underscores why k-anonymity focuses on QIs:\neven innocuous-seeming attributes like age and ZIP code can triangulate someone’s identity when combined.\nNext, we describe how our simulated attacker performs the re-identification.\nAttacker’s Re-identification Strategy (Global Matching)\nHow does our attacker try to re-identify records?\nInstead of using a simple greedy matching (checking each anonymized record independently),\nwe implement a\nglobal optimization strategy\n.\nWe use a\nbipartite assignment solver\n(Google OR-Tools’ linear sum assignment solver) to find the optimal one-to-one matching between anonymized records and original records that best aligns their attributes.\nCost-based Matching\nWe define a cost for matching an anonymized record\naᵢ\nwith an original record\noⱼ\nbased on their differences in quasi-identifiers:\ncost(i, j) = d_age(a_i, o_j) + d_zip(a_i, o_j) + d_sex(a_i, o_j)\nEach\nd\nterm is a distance measure for that attribute.\nIf an anonymized age is a range (due to binning) and the original age falls within that range, the age distance may be zero; if it falls outside, the distance increases.\nIf an anonymized ZIP3 was generalized to\n\"Other\"\n, any specific ZIP from the original will incur a cost when compared to\n\"Other\"\n.\nThese distances capture how well an original record fits the generalized form of an anonymized record.\nLower cost → the two records are more similar across QIs.\nWe then solve for the assignment\nπ(i)\nthat minimizes the total cost of matching all anonymized records to distinct original records:\nmin_π  Σ_i  cost(i, π(i))\nsubject to: each original record is matched at most once\nThis optimization finds the\nbest overall matching\nbetween the two datasets.\nBy considering all records jointly, the attacker avoids making locally optimal but globally inconsistent matches.\nEven if each anonymized record individually has multiple plausible matches, the solver finds a\nglobally consistent\nassignment.\nThe outcome is an assignment pairing most anonymized records with specific original record guesses.\nMeasuring Re-identification Success: Hit@1\nTo evaluate the attack, we use the\nHit@k\nmetric common in information retrieval.\nA “hit” means the correct original record appears within the attacker’s\ntop k\nguesses for an anonymized record.\nIn our case:\nthe solver produces\none\nbest match per anonymized record\n→ effectively Hit@1 only\nSo we focus on\nHit@1\n, the fraction of anonymized records where the attacker’s top guess is correct.\nA Hit@1 of\n50%\nmeans the attacker correctly re-identified half of the individuals on the first guess.\n(Hit@5 would allow up to 5 guesses per record, but we stick with the strictest measure.)\nWith the attack strategy and success metric defined, we now examine how re-identification risk and data utility change as anonymization strength increases.\nResults\nRe-identification Success vs. Anonymity Level\nWe first examine how the attacker’s success rate (\nHit@1\n) changes as the anonymity parameter\nk\nincreases.\nIntuitively, higher k (stronger anonymity) should make re-identification harder.\nOur experiments confirmed this:\nthe attacker’s success drops dramatically as k grows.\nEach cell is the average Hit@1 across trials for a given combination of k (y-axis) and age bin width (x-axis). Success rates plummet as k increases. Notably, there is a sharp drop in attacker success once k is around 5–7, indicating the onset of strong anonymity where the attack loses traction.*\nEven at\nk = 1\n(minimal anonymization), the attacker does\nnot\nget a 100% hit rate.\nThe maximum Hit@1 observed hovered just above\n~50%\n.\nThis is because even in the\nraw data\n, some individuals share the same QI values\n(e.g., multiple people with the same age, ZIP3, and sex),\nso they cannot all be uniquely identified by QIs alone.\nThis sets an\nupper ceiling\non re-identification success.\nAs k increases from\n1 to 5\n, Hit@1 falls gradually.\nBeyond\nk ~ 5–7\n, it\nplummets sharply\n.\nBy\nk ≥ 10\n, the attacker’s top-guess accuracy is very low\n(approaching random chance in many settings).\nSummary:\nraising k dramatically improves privacy, especially after the mid-range threshold where anonymity “kicks in.”\nData Utility Loss as k Increases\nStronger anonymization comes at the cost of\ndata utility\n.\nWe tracked several metrics to quantify how the dataset’s analytical value degrades as k increases:\nZIP Utility:\nMeasures how well the distribution of ZIP3 values is preserved.\nDefined between 0–1, where 1.0 means the anonymized ZIP distribution exactly matches the original.\n(Computed as (1 - \\frac{1}{2} L_1) distance.)\nMean Age Drift:\nThe difference in the average age between anonymized and original data.\nCaptures how much anonymization distorts age information.\nMean Glucose:\nA sanity check for a\nnon-QI variable\nthat should remain unchanged.\nTwo of these metrics,\nZIP Utility\nand\nAge Drift\n, clearly illustrate the non-linear loss of detail as k grows.\nThe line shows that as k increases, the ZIP code distribution retains less and less of its original detail.\nOnce k exceeds about 8, we see a notable drop in ZIP Utility.\nAt k = 16, roughly 25–30% of the geographic granularity is lost.*\nA negative drift means the anonymized data’s average age is lower than the original.\nAt high anonymity levels (k ≈ 20), the mean age is about 3 years lower.\nTop-coding and heavy binning compress the age distribution toward the middle.*\nReassuringly,\nMean Glucose\nremained essentially unchanged across all k values (drift ~0).\nThis confirms that the anonymization procedure targeted only QIs (age, zip, sex) and did not distort unrelated attributes.\nOverall:\nFor small increases in\nk (1–5)\n, utility remains close to original fidelity.\nBeyond\nk ≈ 5–8\n, generalization becomes aggressive and utility drops sharply.\nThis suggests a\n“sweet spot”\nwhere privacy improves significantly while preserving substantial utility, after which additional privacy becomes expensive in terms of information loss.\nThe Privacy–Utility Frontier\nIt is helpful to visualize the inherent trade-off between privacy and utility.\nEach anonymization configuration we tested\n(a specific combination of\nk\nand generalization parameters)\ncan be thought of as a single point in a two-dimensional space:\none axis =\nprivacy outcome\n(e.g., Hit@1 re-identification success)\nthe other axis =\nutility outcome\n(e.g., how many candidate matches remain / how much detail is preserved)\nPlotting all configurations reveals a clear\nprivacy–utility frontier\n.\nEach point represents one anonymization scenario (specific k and parameter settings), plotted by its resulting privacy risk (y-axis: Hit@1 success rate) and a utility indicator (x-axis: number of plausible candidate matches per anonymized record, which correlates with retained information).\nThe plot forms a\ndownward-sloping curve\n.\nConfigurations with\nlower re-identification risk\ninvariably have\nlower data utility\n.\nThe initial part of the curve is\nsteep\n— meaning you can reduce risk significantly with only a small drop in utility.\nThe later part of the curve\nflattens\n— meaning achieving tiny extra privacy gains requires\nlarge utility sacrifices\n.\nIn simpler terms:\nYou can’t have it all.\nPast a certain point, making the data “very anonymous” makes it statistically or analytically blurry.\nThe scatter shows every dataset version lies somewhere on this curve.\nDeciding where to operate is a\npolicy choice\n:\nlow k\n→ high utility, low privacy\nhigh k\n→ high privacy, low utility\nConclusion and Discussion\nOur empirical exploration highlights how increasing\nk-anonymity\nleads to\ndiminishing returns\n.\nFor\nmodest anonymity levels\n(up to around k = 5):\nEach increment in k yields a\nbig drop\nin re-identification risk.\nThe corresponding hit to data utility is\nmild\n.\nBeyond that, however, the trade-off worsens:\nPushing k higher gives\nsmaller and smaller privacy benefits\n.\nMeanwhile, it\nrapidly erodes\nthe granularity and usefulness of the data.\nThis is essentially a manifestation of a\nPareto frontier\n—\nthere comes a point where you must give up\na lot\nof utility to get\na little\nmore privacy.\nAttribute Sensitivity\nDifferent data attributes showed\ndifferent sensitivity\nto anonymization:\nGeographic detail (ZIP3)\ndegraded\nfirst\n.\nMany ZIPs are rare → must be collapsed to\n\"Other\"\nas k grows.\nAge\nwas more resilient but eventually smoothed by\nwide bins\nand\ntop-coding\n→ resulting in shifts such as\na\n3-year drop\nin average age at high k.\nlab_glucose\nremained unchanged.\nSince glucose was\nnot\npart of the QIs, anonymization preserved it.\nThis demonstrates that non-identifying variables can remain intact\neven as identifying information is stripped away.\nThis attribute-by-attribute difference shows that\nutility loss is domain-specific\n.\nSome features lose meaning faster than others under anonymization.\nAbout the Attacker Model\nIt is also worth noting that our attack model was relatively\nbasic\n.\nWe assumed the attacker only knows:\nAnd they use a\nstraightforward optimal matching algorithm\n.\nA more determined adversary might:\nhave\nadditional clues\n(e.g., approximate health measurements)\naccess\nmultiple leaks\nuse\nstatistical models\nto narrow matches\nuse Bayesian linkage, ML-based scoring, or constraint solvers\nSuch an attacker could defeat k-anonymity more often.\nTherefore the Hit@1 rates in our experiment may be\noptimistic\n.\nReal-world re-identification risk could be\nhigher\n.\nThis highlights that anonymization should\nnot\nbe:\na one-time, set-and-forget protection mechanism.\nYou must consider\nevolving threat models\nand possibly combine k-anonymity with other techniques:\nnoise addition\nperturbation\ndifferential privacy\nsynthetic data generation\nsecure linkage systems\nFinding the Balance\nUltimately, choosing an anonymization level is about\nbalancing privacy risk against data usability\n.\nOur experiment puts\nconcrete numbers\non that balance:\nThe initial drop in re-id risk (as k rises from 1→5) is\nencouraging\n.\nIt means we can significantly protect identities\nwithout\nimmediately destroying utility.\nBut the flattening of the curve at higher k reminds us that\naggressive anonymization\nyields minimal extra privacy at\nhuge cost\n.\nDecision-makers should consider what level of risk is acceptable\ngiven the\npurpose\nof the data.\nFor many cases:\nKey Takeaways\nk-anonymity trades precision for privacy\n.\nGeneralization and suppression remove detail from QIs.\nPrivacy gains are strong at first, then plateau\n.\nBeyond mid-range k, utility collapses faster than privacy improves.\nUtility loss is nonlinear and varies by attribute\n.\nSparse attributes like ZIP lose meaning earlier.\nNon-identifying attributes can remain intact\n.\nGood for preserving analytical value.\nPast moderate k, returns diminish greatly\n.\nMore anonymity → minimal privacy gain, major utility loss.\nIn conclusion:\nEffective anonymization is about finding the\nbalance\n:\nprotecting individuals without rendering data barren for analysis.\nOur findings illustrate that balance clearly for this dataset under different settings.",
    "description": "",
    "id": null,
    "short_id": null,
    "url": "https://www.testingbranch.com/re_identification/",
    "score": "5",
    "tags": "privacy",
    "domain": "testingbranch.com",
    "author": "Mbat",
    "author_url": "Mbat",
    "time": "14 hours ago",
    "comments_text": "4 comments",
    "comments_url": "4 comments",
    "description_hint": null
  },
  "cached_at": "2026-02-12T05:05:35.811658"
}
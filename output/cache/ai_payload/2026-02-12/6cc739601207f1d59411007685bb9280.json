{
  "url": "https://www.labs.greynoise.io/grimoire/2026-02-10-telnet-falls-silent/",
  "payload": {
    "title": "The Day the Telnet Died",
    "content": "A long, long time ago\nI can still remember how a protocol\nused to make me smile\nAnd I knew if I had my chance\nThat I could make those botnets dance\nAnd maybe they'd be happy for a while\n\nBut January made me shiver\nWith every packet I tried to deliver\nBad news on the backbone\nI couldn't scan a single ASN\n\nI can't remember if I cried\nWhen my -f root hit an ACL line\nBut something touched me deep inside\nThe day the telnet died\n\nSo bye, bye mass spreading Mirai\nDrove my SYNs down on the fiber line\nBut the fiber line was dry\nAnd good old bots were passing creds in the clear and dry\nSingin' this'll be the day that I die\nThis'll be the day that I die\nOn January 14, 2026, at approximately 21:00 UTC, something changed in the internet’s plumbing. The GreyNoise Global Observation Grid recorded a sudden, sustained collapse in global telnet traffic — not a gradual decline, not scanner attrition, not a data pipeline problem, but a step function. One hour, ~74,000 sessions. The next, ~22,000. By the following hour, we were down to ~11,000 and the floor held.\nSix days later, on\nJanuary 20\n, the security advisory for CVE-2026-24061 hit oss-security. By\nJanuary 26\n, CISA had added it to the KEV catalog.\nWe wrote about the\nfirst 18 hours of exploitation activity\nback on January 22. This post is about something different: the structural change in global telnet traffic that preceded the CVE, and why we think the two events may not be independent.\nThe Drop\nFrom December 1, 2025 through January 14, 2026, GreyNoise observed an average of ~914,000 non-spoofable telnet sessions per day across 51.2 million total sessions — let’s call that the “baseline”.\nOn January 14 at 21:00 UTC, hourly volume dropped 65% in a single tick. Within two hours it had fallen 83% below baseline. The new average settled around ~373,000 sessions/day — a\n59% sustained reduction\nthat persists through the time of writing (February 10).\nThis wasn’t a taper. The hourly data around the inflection point tells the story:\nJan 14, 19:00\n73,900\nNormal baseline\nJan 14, 20:00\n64,722\nNormal baseline\nJan 14, 21:00\n22,460\n65% drop in one hour\nJan 14, 22:00\n11,325\n83% below baseline\nJan 14, 23:00\n11,147\nNew floor established\nJan 15, 00:00\n12,089\nSustained at reduced level\nThat kind of step function — propagating within a single hour window — reads as a configuration change on routing infrastructure, not behavioral drift in scanning populations.\nWhat Went Silent\nEighteen ASNs with significant pre-drop telnet volume (>50K sessions each) went to absolute zero after January 15. Some of the names that stand out:\nVultr\n(AS20473) — 382K pre-drop sessions, then nothing\nCox Communications\n(AS22773) — 150K sessions, gone\nCharter/Spectrum\n(AS20115) — 141K sessions, gone\nBT/British Telecom\n(AS2856) — 127K sessions, gone\nFive entire countries vanished from GreyNoise telnet data: Zimbabwe, Ukraine, Canada, Poland, and Egypt. Not reduced —\nzero\n.\nMeanwhile, the major cloud providers were largely unaffected or even increased. AWS went\nup\n78%. Contabo\nup\n90%. DigitalOcean essentially flat at +3%. Cloud providers have extensive private peering at major IXPs that bypasses traditional transit backbone paths. Residential and enterprise ISPs typically don’t.\nWhere’s the Filter?\nThe pattern points toward one or more North American Tier 1 transit providers implementing port 23 filtering:\nThe timing — 21:00 UTC, which is 16:00 EST — is consistent with a US-based maintenance window. US residential ISPs (Cox, Charter, Comcast at -74%) were devastated while cloud providers on the same continent peered around whatever changed. Verizon/UUNET (AS701) dropped 79%, and as a major Tier 1 backbone, that’s consistent with it either being the filtering entity or sitting directly upstream of one. The 21% residual traffic on AS701 would represent paths that don’t transit the filtered links.\nCountries that rely on transatlantic or transpacific backbone routes to reach US-hosted infrastructure got hit hardest. Countries with strong direct European peering (France at +18%, Germany at -1%) were essentially unaffected.\nThe Chinese backbone providers (China Telecom and China Unicom) both dropped ~59%, uniformly. That uniformity suggests the filter sits on the US side of transpacific links rather than within China. If this were a Chinese firewall action, we’d expect asymmetric impact across Chinese carriers and a harder cutoff.\nThen Came the CVE\nCVE-2026-24061 is a critical (CVSS 9.8) authentication bypass in GNU Inetutils telnetd. The flaw is an argument injection in how telnetd handles the\nUSER\nenvironment variable during telnet option negotiation. An attacker sends\n-f root\nas the username value, and\nlogin(1)\nobediently skips authentication, handing over a root shell. No credentials required. No user interaction. The vulnerable code was introduced in a 2015 commit and sat undiscovered for nearly 11 years.\nThe timeline:\nJan 14, 21:00 UTC\nTelnet backbone drop begins\nJan 20\nCVE-2026-24061 advisory posted to oss-security\nJan 21\nNVD entry published; GreyNoise tag deployed; first exploitation observed\nJan 22\nGreyNoise Grimoire post\non initial 18 hours of exploitation\nJan 26\nCISA adds CVE-2026-24061 to KEV catalog\nThe six-day gap between the telnet drop and the public CVE disclosure is the interesting part. On its face, the drop can’t have been\ncaused\nby the CVE disclosure, because the drop happened first. But “caused by” isn’t the only relationship worth considering.\nThe Supposition\nResponsible disclosure timelines don’t start at publication. The researcher who found this (\ncredited as Kyu Neushwaistein / Carlos Cortes Alvarez\n) reported the flaw on January 19, per public sources. But the coordination that leads to patches being ready, advisories being drafted, and CISA being prepared to add something to the KEV within six days of publication typically starts earlier than the day before disclosure.\nHere’s what we think may have happened: advance notification of a trivially exploitable, unauthenticated root-access vulnerability affecting telnet daemons reached parties with the ability to act on it at the infrastructure level. A backbone or transit provider — possibly responding to a coordinated request, possibly acting on their own assessment — implemented port 23 filtering on transit links. The filtering went live on January 14. The public disclosure followed on January 20.\nThis would explain:\nThe timing gap (advance notification → infrastructure response → public disclosure)\nThe specificity of the filtering (port 23/TCP, not a general routing change)\nThe topology of impact (transit-dependent paths affected, direct-peering paths not)\nThe sustained nature (the filter is still in place weeks later)\nWe can’t prove this. The backbone drop could be entirely coincidental — ISPs have been slowly moving toward filtering legacy insecure protocols for years (ref: Wannacry), and January 14 could simply have been when someone’s change control ticket finally got executed. Correlation, temporal proximity, and a plausible mechanism\nabsolutely do not\nequal causation.\nBut the combination of a Tier 1 backbone implementing what appears to be port 23 filtering, followed six days later by the disclosure of a trivially exploitable root-access telnet vulnerability, followed four days after\nthat\nby a CISA KEV listing, is worth documenting and considering.\nWhat the Post-Drop World Looks Like\nThe telnet landscape after January 14 shows a recurring sawtooth pattern — periodic spikes followed by troughs (e.g., January 28 at 806K sessions, then January 30 at 191K). This could indicate intermittent filter application, routing flaps around the filtering infrastructure, or scanner campaigns that happen to use paths not affected by the filter.\nThe weekly averages tell the sustained story:\nDec 01\n1,086,744\n119%\nJan 05\n985,699\n108%\nJan 19\n363,184\n40%\nJan 26\n407,182\n45%\nFeb 02\n322,606\n35%\nWe’re now operating at roughly a third of the pre-drop baseline, and the trend is still slightly downward.\nPractical Implications\nIf you’re running GNU Inetutils telnetd anywhere — and given the 11-year window, there are plenty of embedded systems, network appliances, and legacy Linux installations where it’s still likely present — patch to version 2.7-2 or later, or disable the service entirely. The CISA KEV remediation deadline for federal agencies is February 16, 2026. As noted, GreyNoise observed exploitation attempts\nwithin hours of disclosure\nand the campaign peaked at ~2,600 sessions/day in early February before tapering off.\nIf you’re a network operator and you haven’t already filtered port 23 at your border, the backbone-level filtering we’ve documented here suggests the industry is moving in that direction regardless. Someone upstream of a significant chunk of the internet’s transit infrastructure apparently decided telnet traffic isn’t worth carrying anymore. That’s probably the right call.\nIf you know anything about this (or was the brave soul who implemented it), drop us a line at\nresearch@greynoise.io\n.",
    "description": "",
    "url": "https://www.labs.greynoise.io/grimoire/2026-02-10-telnet-falls-silent/"
  },
  "cached_at": "2026-02-12T05:02:08.429886"
}
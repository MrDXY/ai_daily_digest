{
  "url": "https://github.com/google/langextract",
  "payload": {
    "title": "google /langextract",
    "content": "readme: LangExtractTable of ContentsIntroductionWhy LangExtract?Quick StartInstallationAPI Key Setup for Cloud ModelsAdding Custom Model ProvidersUsing OpenAI ModelsUsing Local LLMs with OllamaMore ExamplesRomeo and JulietFull Text ExtractionMedication ExtractionRadiology Report Structuring: RadExtractCommunity ProvidersContributingTestingDisclaimerIntroductionLangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.Why LangExtract?Precise Source Grounding:Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.Reliable Structured Outputs:Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.Optimized for Long Documents:Overcomes the \"needle-in-a-haystack\" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.Interactive Visualization:Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.Flexible LLM Support:Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.Adaptable to Any Domain:Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.Leverages LLM World Knowledge:Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.Quick StartNote:Using cloud-hosted models like Gemini requires an API key. See theAPI Key Setupsection for instructions on how to get and configure your key.Extract structured information with just a few lines of code.1. Define Your Extraction TaskFirst, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.importlangextractaslximporttextwrap# 1. Define the prompt and extraction rulesprompt=textwrap.dedent(\"\"\"\\Extract characters, emotions, and relationships in order of appearance.Use exact text for extractions. Do not paraphrase or overlap entities.Provide meaningful attributes for each entity to add context.\"\"\")# 2. Provide a high-quality example to guide the modelexamples=[lx.data.ExampleData(text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",extractions=[lx.data.Extraction(extraction_class=\"character\",extraction_text=\"ROMEO\",attributes={\"emotional_state\":\"wonder\"}\n            ),lx.data.Extraction(extraction_class=\"emotion\",extraction_text=\"But soft!\",attributes={\"feeling\":\"gentle awe\"}\n            ),lx.data.Extraction(extraction_class=\"relationship\",extraction_text=\"Juliet is the sun\",attributes={\"type\":\"metaphor\"}\n            ),\n        ]\n    )\n]Note:Examples drive model behavior. Eachextraction_textshould ideally be verbatim from the example'stext(no paraphrasing), listed in order of appearance. LangExtract raisesPrompt alignmentwarnings by default if examples don't follow this pattern—resolve these for best results.2. Run the ExtractionProvide your input text and the prompt materials to thelx.extractfunction.# The input text to be processedinput_text=\"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"# Run the extractionresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gemini-2.5-flash\",\n)Model Selection:gemini-2.5-flashis the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning,gemini-2.5-promay provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See therate-limit documentationfor details.Model Lifecycle: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult theofficial model version documentationto stay informed about the latest stable and legacy versions.3. Visualize the ResultsThe extractions can be saved to a.jsonlfile, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.# Save the results to a JSONL filelx.io.save_annotated_documents([result],output_name=\"extraction_results.jsonl\",output_dir=\".\")# Generate the visualization from the filehtml_content=lx.visualize(\"extraction_results.jsonl\")withopen(\"visualization.html\",\"w\")asf:ifhasattr(html_content,'data'):f.write(html_content.data)# For Jupyter/Colabelse:f.write(html_content)This creates an animated and interactive HTML file:Note on LLM Knowledge Utilization:This example demonstrates extractions that stay close to the text evidence - extracting \"longing\" for Lady Juliet's emotional state and identifying \"yearning\" from \"gazed longingly at the stars.\" The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding\"identity\": \"Capulet family daughter\"or\"literary_context\": \"tragic heroine\"). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.Scaling to Longer DocumentsFor larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:# Process Romeo & Juliet directly from Project Gutenbergresult=lx.extract(text_or_documents=\"https://www.gutenberg.org/files/1513/1513-0.txt\",prompt_description=prompt,examples=examples,model_id=\"gemini-2.5-flash\",extraction_passes=3,# Improves recall through multiple passesmax_workers=20,# Parallel processing for speedmax_char_buffer=1000# Smaller contexts for better accuracy)This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file.See the fullRomeo and Julietextraction example →for detailed results and performance insights.Vertex AI Batch ProcessingSave costs on large-scale tasks by enabling Vertex AI Batch API:language_model_params={\"vertexai\": True, \"batch\": {\"enabled\": True}}.See an example of the Vertex AI Batch API usage inthis example.InstallationFrom PyPIpip install langextractRecommended for most users. For isolated environments, consider using a virtual environment:python -m venv langextract_envsourcelangextract_env/bin/activate#On Windows: langextract_env\\Scripts\\activatepip install langextractFrom SourceLangExtract uses modern Python packaging withpyproject.tomlfor dependency management:Installing with-eputs the package in development mode, allowing you to modify the code without reinstalling.git clone https://github.com/google/langextract.gitcdlangextract#For basic installation:pip install -e.#For development (includes linting tools):pip install -e\".[dev]\"#For testing (includes pytest):pip install -e\".[test]\"Dockerdocker build -t langextract.docker run --rm -e LANGEXTRACT_API_KEY=\"your-api-key\"langextract python your_script.pyAPI Key Setup for Cloud ModelsWhen using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to\nset up an API key. On-device models don't require an API key. For developers\nusing local LLMs, LangExtract offers built-in support for Ollama and can be\nextended to other third-party APIs by updating the inference endpoints.API Key SourcesGet API keys from:AI Studiofor Gemini modelsVertex AIfor enterprise useOpenAI Platformfor OpenAI modelsSetting up API key in your environmentOption 1: Environment VariableexportLANGEXTRACT_API_KEY=\"your-api-key-here\"Option 2: .env File (Recommended)Add your API key to a.envfile:#Add API key to .env filecat>>.env<<'EOF'LANGEXTRACT_API_KEY=your-api-key-hereEOF#Keep your API key secureecho'.env'>>.gitignoreIn your Python code:importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\")Option 3: Direct API Key (Not Recommended for Production)You can also provide the API key directly in your code, though this is not recommended for production use:result=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\",api_key=\"your-api-key-here\"# Only use this for testing/development)Option 4: Vertex AI (Service Accounts)UseVertex AIfor authentication with service accounts:result=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\",language_model_params={\"vertexai\":True,\"project\":\"your-project-id\",\"location\":\"global\"# or regional endpoint}\n)Adding Custom Model ProvidersLangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.Add new model support independently of the core libraryDistribute your provider as a separate Python packageKeep custom dependencies isolatedOverride or extend built-in providers via priority-based resolutionSee the detailed guide inProvider System Documentationto learn how to:Register a provider with@registry.register(...)Publish an entry point for discoveryOptionally provide a schema withget_schema_class()for structured outputIntegrate with the factory viacreate_model(...)Using OpenAI ModelsLangExtract supports OpenAI models (requires optional dependency:pip install langextract[openai]):importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gpt-4o\",# Automatically selects OpenAI providerapi_key=os.environ.get('OPENAI_API_KEY'),fence_output=True,use_schema_constraints=False)Note: OpenAI models requirefence_output=Trueanduse_schema_constraints=Falsebecause LangExtract doesn't implement schema constraints for OpenAI yet.Using Local LLMs with OllamaLangExtract supports local inference using Ollama, allowing you to run models without API keys:importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gemma2:2b\",# Automatically selects Ollama providermodel_url=\"http://localhost:11434\",fence_output=False,use_schema_constraints=False)Quick setup:Install Ollama fromollama.com, runollama pull gemma2:2b, thenollama serve.For detailed installation, Docker setup, and examples, seeexamples/ollama/.More ExamplesAdditional examples of LangExtract in action:Romeo and JulietFull Text ExtractionLangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text ofRomeo and Julietfrom Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.ViewRomeo and JulietFull Text Example →Medication ExtractionDisclaimer:This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.View Medication Examples →Radiology Report Structuring: RadExtractExplore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.View RadExtract Demo →Community ProvidersExtend LangExtract with custom model providers! Check out ourCommunity Provider Pluginsregistry to discover providers created by the community or add your own.For detailed instructions on creating a provider plugin, see theCustom Provider Plugin Example.ContributingContributions are welcome! SeeCONTRIBUTING.mdto get started\nwith development, testing, and pull requests. You must sign aContributor License Agreementbefore submitting patches.TestingTo run tests locally from the source:#Clone the repositorygit clone https://github.com/google/langextract.gitcdlangextract#Install with test dependenciespip install -e\".[test]\"#Run all testspytest testsOr reproduce the full CI matrix locally with tox:tox#runs pylint + pytest on Python 3.10 and 3.11Ollama Integration TestingIf you have Ollama installed locally, you can run integration tests:#Test Ollama integration (requires Ollama running with gemma2:2b model)tox -e ollama-integrationThis test will automatically detect if Ollama is available and run real inference tests.DevelopmentCode FormattingThis project uses automated formatting tools to maintain consistent code style:#Auto-format all code./autoformat.sh#Or run formatters separatelyisort langextract tests --profile google --line-length 80\npyink langextract tests --config pyproject.tomlPre-commit HooksFor automatic formatting checks:pre-commit install#One-time setuppre-commit run --all-files#Manual runLintingRun linting before submitting PRs:pylint --rcfile=.pylintrc langextract testsSeeCONTRIBUTING.mdfor full development guidelines.DisclaimerThis is not an officially supported Google product. If you use\nLangExtract in production or publications, please cite accordingly and\nacknowledge usage. Use is subject to theApache 2.0 License.\nFor health-related applications, use of LangExtract is also subject to theHealth AI Developer Foundations Terms of Use.Happy Extracting!",
    "description": "A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.",
    "url": "https://github.com/google/langextract",
    "language": "Python",
    "stars": "30,657",
    "forks": "2,047",
    "stars_today": "3,186 stars today",
    "readme": "LangExtractTable of ContentsIntroductionWhy LangExtract?Quick StartInstallationAPI Key Setup for Cloud ModelsAdding Custom Model ProvidersUsing OpenAI ModelsUsing Local LLMs with OllamaMore ExamplesRomeo and JulietFull Text ExtractionMedication ExtractionRadiology Report Structuring: RadExtractCommunity ProvidersContributingTestingDisclaimerIntroductionLangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.Why LangExtract?Precise Source Grounding:Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.Reliable Structured Outputs:Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.Optimized for Long Documents:Overcomes the \"needle-in-a-haystack\" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.Interactive Visualization:Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.Flexible LLM Support:Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.Adaptable to Any Domain:Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.Leverages LLM World Knowledge:Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.Quick StartNote:Using cloud-hosted models like Gemini requires an API key. See theAPI Key Setupsection for instructions on how to get and configure your key.Extract structured information with just a few lines of code.1. Define Your Extraction TaskFirst, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.importlangextractaslximporttextwrap# 1. Define the prompt and extraction rulesprompt=textwrap.dedent(\"\"\"\\Extract characters, emotions, and relationships in order of appearance.Use exact text for extractions. Do not paraphrase or overlap entities.Provide meaningful attributes for each entity to add context.\"\"\")# 2. Provide a high-quality example to guide the modelexamples=[lx.data.ExampleData(text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",extractions=[lx.data.Extraction(extraction_class=\"character\",extraction_text=\"ROMEO\",attributes={\"emotional_state\":\"wonder\"}\n            ),lx.data.Extraction(extraction_class=\"emotion\",extraction_text=\"But soft!\",attributes={\"feeling\":\"gentle awe\"}\n            ),lx.data.Extraction(extraction_class=\"relationship\",extraction_text=\"Juliet is the sun\",attributes={\"type\":\"metaphor\"}\n            ),\n        ]\n    )\n]Note:Examples drive model behavior. Eachextraction_textshould ideally be verbatim from the example'stext(no paraphrasing), listed in order of appearance. LangExtract raisesPrompt alignmentwarnings by default if examples don't follow this pattern—resolve these for best results.2. Run the ExtractionProvide your input text and the prompt materials to thelx.extractfunction.# The input text to be processedinput_text=\"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"# Run the extractionresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gemini-2.5-flash\",\n)Model Selection:gemini-2.5-flashis the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning,gemini-2.5-promay provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See therate-limit documentationfor details.Model Lifecycle: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult theofficial model version documentationto stay informed about the latest stable and legacy versions.3. Visualize the ResultsThe extractions can be saved to a.jsonlfile, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.# Save the results to a JSONL filelx.io.save_annotated_documents([result],output_name=\"extraction_results.jsonl\",output_dir=\".\")# Generate the visualization from the filehtml_content=lx.visualize(\"extraction_results.jsonl\")withopen(\"visualization.html\",\"w\")asf:ifhasattr(html_content,'data'):f.write(html_content.data)# For Jupyter/Colabelse:f.write(html_content)This creates an animated and interactive HTML file:Note on LLM Knowledge Utilization:This example demonstrates extractions that stay close to the text evidence - extracting \"longing\" for Lady Juliet's emotional state and identifying \"yearning\" from \"gazed longingly at the stars.\" The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding\"identity\": \"Capulet family daughter\"or\"literary_context\": \"tragic heroine\"). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.Scaling to Longer DocumentsFor larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:# Process Romeo & Juliet directly from Project Gutenbergresult=lx.extract(text_or_documents=\"https://www.gutenberg.org/files/1513/1513-0.txt\",prompt_description=prompt,examples=examples,model_id=\"gemini-2.5-flash\",extraction_passes=3,# Improves recall through multiple passesmax_workers=20,# Parallel processing for speedmax_char_buffer=1000# Smaller contexts for better accuracy)This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file.See the fullRomeo and Julietextraction example →for detailed results and performance insights.Vertex AI Batch ProcessingSave costs on large-scale tasks by enabling Vertex AI Batch API:language_model_params={\"vertexai\": True, \"batch\": {\"enabled\": True}}.See an example of the Vertex AI Batch API usage inthis example.InstallationFrom PyPIpip install langextractRecommended for most users. For isolated environments, consider using a virtual environment:python -m venv langextract_envsourcelangextract_env/bin/activate#On Windows: langextract_env\\Scripts\\activatepip install langextractFrom SourceLangExtract uses modern Python packaging withpyproject.tomlfor dependency management:Installing with-eputs the package in development mode, allowing you to modify the code without reinstalling.git clone https://github.com/google/langextract.gitcdlangextract#For basic installation:pip install -e.#For development (includes linting tools):pip install -e\".[dev]\"#For testing (includes pytest):pip install -e\".[test]\"Dockerdocker build -t langextract.docker run --rm -e LANGEXTRACT_API_KEY=\"your-api-key\"langextract python your_script.pyAPI Key Setup for Cloud ModelsWhen using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to\nset up an API key. On-device models don't require an API key. For developers\nusing local LLMs, LangExtract offers built-in support for Ollama and can be\nextended to other third-party APIs by updating the inference endpoints.API Key SourcesGet API keys from:AI Studiofor Gemini modelsVertex AIfor enterprise useOpenAI Platformfor OpenAI modelsSetting up API key in your environmentOption 1: Environment VariableexportLANGEXTRACT_API_KEY=\"your-api-key-here\"Option 2: .env File (Recommended)Add your API key to a.envfile:#Add API key to .env filecat>>.env<<'EOF'LANGEXTRACT_API_KEY=your-api-key-hereEOF#Keep your API key secureecho'.env'>>.gitignoreIn your Python code:importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\")Option 3: Direct API Key (Not Recommended for Production)You can also provide the API key directly in your code, though this is not recommended for production use:result=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\",api_key=\"your-api-key-here\"# Only use this for testing/development)Option 4: Vertex AI (Service Accounts)UseVertex AIfor authentication with service accounts:result=lx.extract(text_or_documents=input_text,prompt_description=\"Extract information...\",examples=[...],model_id=\"gemini-2.5-flash\",language_model_params={\"vertexai\":True,\"project\":\"your-project-id\",\"location\":\"global\"# or regional endpoint}\n)Adding Custom Model ProvidersLangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.Add new model support independently of the core libraryDistribute your provider as a separate Python packageKeep custom dependencies isolatedOverride or extend built-in providers via priority-based resolutionSee the detailed guide inProvider System Documentationto learn how to:Register a provider with@registry.register(...)Publish an entry point for discoveryOptionally provide a schema withget_schema_class()for structured outputIntegrate with the factory viacreate_model(...)Using OpenAI ModelsLangExtract supports OpenAI models (requires optional dependency:pip install langextract[openai]):importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gpt-4o\",# Automatically selects OpenAI providerapi_key=os.environ.get('OPENAI_API_KEY'),fence_output=True,use_schema_constraints=False)Note: OpenAI models requirefence_output=Trueanduse_schema_constraints=Falsebecause LangExtract doesn't implement schema constraints for OpenAI yet.Using Local LLMs with OllamaLangExtract supports local inference using Ollama, allowing you to run models without API keys:importlangextractaslxresult=lx.extract(text_or_documents=input_text,prompt_description=prompt,examples=examples,model_id=\"gemma2:2b\",# Automatically selects Ollama providermodel_url=\"http://localhost:11434\",fence_output=False,use_schema_constraints=False)Quick setup:Install Ollama fromollama.com, runollama pull gemma2:2b, thenollama serve.For detailed installation, Docker setup, and examples, seeexamples/ollama/.More ExamplesAdditional examples of LangExtract in action:Romeo and JulietFull Text ExtractionLangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text ofRomeo and Julietfrom Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.ViewRomeo and JulietFull Text Example →Medication ExtractionDisclaimer:This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.View Medication Examples →Radiology Report Structuring: RadExtractExplore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.View RadExtract Demo →Community ProvidersExtend LangExtract with custom model providers! Check out ourCommunity Provider Pluginsregistry to discover providers created by the community or add your own.For detailed instructions on creating a provider plugin, see theCustom Provider Plugin Example.ContributingContributions are welcome! SeeCONTRIBUTING.mdto get started\nwith development, testing, and pull requests. You must sign aContributor License Agreementbefore submitting patches.TestingTo run tests locally from the source:#Clone the repositorygit clone https://github.com/google/langextract.gitcdlangextract#Install with test dependenciespip install -e\".[test]\"#Run all testspytest testsOr reproduce the full CI matrix locally with tox:tox#runs pylint + pytest on Python 3.10 and 3.11Ollama Integration TestingIf you have Ollama installed locally, you can run integration tests:#Test Ollama integration (requires Ollama running with gemma2:2b model)tox -e ollama-integrationThis test will automatically detect if Ollama is available and run real inference tests.DevelopmentCode FormattingThis project uses automated formatting tools to maintain consistent code style:#Auto-format all code./autoformat.sh#Or run formatters separatelyisort langextract tests --profile google --line-length 80\npyink langextract tests --config pyproject.tomlPre-commit HooksFor automatic formatting checks:pre-commit install#One-time setuppre-commit run --all-files#Manual runLintingRun linting before submitting PRs:pylint --rcfile=.pylintrc langextract testsSeeCONTRIBUTING.mdfor full development guidelines.DisclaimerThis is not an officially supported Google product. If you use\nLangExtract in production or publications, please cite accordingly and\nacknowledge usage. Use is subject to theApache 2.0 License.\nFor health-related applications, use of LangExtract is also subject to theHealth AI Developer Foundations Terms of Use.Happy Extracting!"
  },
  "cached_at": "2026-02-12T04:57:57.718837"
}
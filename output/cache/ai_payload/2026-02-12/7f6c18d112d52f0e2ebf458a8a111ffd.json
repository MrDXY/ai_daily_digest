{
  "url": "https://z.ai/blog/glm-5",
  "payload": {
    "title": "GLM-5: Targeting complex systems engineering and long-horizon agentic tasks",
    "content": "We are launching GLM-5, targeting complex systems engineering and long-horizon agentic tasks. Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI). Compared to GLM-4.5, GLM-5 scales from 355B parameters (32B active) to 744B parameters (40B active), and increases pre-training data from 23T to 28.5T tokens. GLM-5 also integrates DeepSeek Sparse Attention (DSA), significantly reducing deployment cost while preserving long-context capacity.\nReinforcement learning aims to bridge the gap between competence and excellence in pre-trained models. However, deploying it at scale for LLMs is a challenge due to RL training inefficiency. To this end, we developed\nslime\n, a novel\nasynchronous RL infrastructure\nthat substantially improves training throughput and efficiency, enabling more fine-grained post-training iterations. With advances in both pre-training and post-training, GLM-5 delivers significant improvement compared to GLM-4.7 across a wide range of academic benchmarks and achieves best-in-class performance among all open-source models in the world on reasoning, coding, and agentic tasks,  closing the gap with frontier models.\nGLM-5 is designed for complex systems engineering and long-horizon agentic tasks. On our internal evaluation suite CC-Bench-V2, GLM-5 significantly outperforms GLM-4.7 across frontend, backend, and long-horizon tasks, narrowing the gap to Claude Opus 4.5.\nOn\nVending Bench 2\n, a benchmark that measures long-term operational capability, GLM-5 ranks #1 among open-source models. Vending Bench 2 requires the model to run a simulated vending machine business over a one-year horizon; GLM-5 finishes with a final account balance of $4,432, approaching Claude Opus 4.5 and demonstrating strong long-term planning and resource management.\nGLM-5 is open-sourced on\nHugging Face\nand\nModelScope\n, with model weights released under the MIT License. GLM-5 is also available on developer platform\napi.z.ai\nand\nBigModel.cn\n, with compatibility with Claude Code and OpenClaw. You can also try it for free on\nZ.ai\n.\n*: refers to their scores of full set.\n†: A\nverified version\nof Terminal-Bench 2.0 that fixes some ambiguous instructions.\nSee footnote for more evaluation details.\nOffice\nFoundation models are moving from “chat” to “work,” much like Office tools for knowledge workers and programming tools for engineers.\nGLM-4.5 is our first step for reasoning, coding, and agent, enabling the model to complete complex tasks. With GLM-5, we further enhance complex systems engineering and long-horizon agent capabilities. GLM-5 can turn text or source materials directly into .docx, .pdf, and .xlsx files—PRDs, lesson plans, exams, spreadsheets, financial reports, run sheets, menus, and more—delivered end-to-end as ready-to-use documents.\nOur official application,\nZ.ai\nis rolling out an Agent mode with built-in skills for PDF / Word / Excel creation, supporting multi-turn collaboration and turning outputs into real deliverables.\nWestbrook High School Football Sponsorship Proposal\nNVIDIA Equity Research Report\nGoogle Earnings Review\nYou are writing a visually engaging and well-structured sponsorship proposal intended to be delivered as a DOC document.\nAuthor background:\nThe proposal is written on behalf of a U.S. high school student council.\nPurpose of the document:\nThe goal of this document is to present a clear and compelling proposal to potential sponsors in order to secure financial sponsorship for an upcoming school football game or football season.\nThe proposal should:\nIntroduce the football event and its significance within the school and local community\nExplain how sponsorship funds will be used\nClearly outline sponsorship opportunities and benefits for sponsors\nDemonstrate why sponsoring this event provides meaningful visibility and community engagement\nTarget audience:\nLocal businesses, community organizations, and potential corporate sponsors interested in youth sports, education, and community involvement.\n────────────────\nOverall positioning:\nThis is a formal but youth-led sponsorship proposal.\nThe tone should be:\nPositive, energetic, and respectful\nProfessional but approachable\nCommunity-oriented and sincere\nAvoid exaggerated claims or overly commercial language.\n────────────────\nRequired structure and content:\nIntroduction\nBrief introduction of the school, student council, and football program\nPurpose of the sponsorship request\nAbout the Football Event\nDescription of the football game or season\nImportance of football to school spirit, teamwork, and student life\nExpected attendance (students, families, community members)\nUse of Sponsorship Funds\nHow sponsorship money will support the event (equipment, facilities, uniforms, event operations, etc.)\nEmphasis on student benefit and community impact\nSponsorship Opportunities\nDifferent sponsorship levels (e.g., Gold, Silver, Bronze)\nWhat sponsors receive at each level (logo placement, announcements, banners, programs, social media mentions, etc.)\nBenefits to Sponsors\nBrand visibility within the school and local community\nPositive association with youth development and education\nOpportunities for long-term partnership\nConclusion and Call to Action\nExpression of appreciation\nClear next steps for interested sponsors\n────────────────\nVisual and design requirements (very important):\nThe document must be visually rich and engaging.\nInclude and reference visual elements such as:\nPhotos or image placeholders of football games, players, or school spirit events\nTables comparing sponsorship levels and benefits\nHighlight boxes or callouts for key information\nUse captions such as:\n\"Image: Our school football team during a home game\"\n\"Table: Sponsorship levels and benefits overview\"\nVisuals should support clarity and excitement, not decoration only.\n────────────────\nColor and style guidelines:\nUse a colorful, energetic, and school-friendly visual style.\nSuggested color palette (can be adapted to school colors):\nPrimary color (section titles): deep school color (e.g., navy blue or maroon)\nSecondary color (subsections): lighter complementary color\nAccent colors: bright but tasteful tones (e.g., gold, orange, or light blue)\nBody text: dark gray or black\nTable headers / highlight boxes: light, cheerful background colors\nColor usage rules:\nUse color to create visual hierarchy and excitement.\nAvoid overly dark or dull designs.\nEnsure good contrast for readability.\n────────────────\nWriting and layout constraints:\nUse clear, simple, and friendly language.\nParagraphs should be short and easy to read.\nDo NOT insert line breaks in the middle of sentences.\nUse bullet points and tables where appropriate.\nEnsure the document reads well both on screen and when printed.\nQuality bar:\nThe document should look like a well-prepared student council sponsorship proposal.\nSponsors should clearly understand the event, the value of sponsorship, and how to get involved.\nThe final output should be ready to be shared as a DOC file without further editing.\nImage should be in the center.\nDocument (.docx) generated by GLM-5\nGetting started with GLM-5\nUse GLM-5 with GLM Coding Plan\nTry\nGLM-5\nin your favorite coding agents—\nClaude Code, OpenCode, Kilo Code, Roo Code, Cline, Droid\n, and more.\nhttps://docs.z.ai/devpack/overview\nFor GLM Coding Plan subscribers:\nDue to limited compute capacity, we’re rolling out GLM-5 to Coding Plan users\ngradually\n.\nMax plan users:\nYou can enable GLM-5 now by updating the model name to\n\"GLM-5\"\n(e.g. in\n~/.claude/settings.json\nfor Claude Code).\nOther plan tiers:\nSupport will be added progressively as the rollout expands.\nQuota note:\nRequests to GLM-5 consume\nmore plan quota\nthan GLM-4.7.\nPrefer a GUI? We offer\nZ Code\n—an agentic development environment that lets you control (even remotely) multiple agents and have them collaborate on complex tasks.\nStart building now:\nhttps://z.ai/subscribe\nUse GLM-5 with OpenClaw\nBeyond coding agents, GLM-5 also supports\nOpenClaw\n—a framework that turns GLM-5 into a personal assistant that can\noperate across apps and devices\n, not just chat.\nOpenClaw is included in GLM Coding Plan. See the\nguidance\n.\nChat with GLM-5 on Z.ai\nGLM-5 is accessible through\nZ.ai\n. Manually  change the model option to\nGLM-5\n, if the system does not automatically do so. We offer both Chat and Agent mode for GLM-5:\nChat Mode\n: Instant response, interactive chat, lightweight delivery\nAgent Mode\n: Multiple tools, diverse skills, delivering results directly\nServe GLM-5 Locally\nThe model weights of GLM-5 are publicly available on\nHuggingFace\nand\nModelScope\n. For local deployment, GLM-5 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available at the official GitHub repository.\nWe also support deploying GLM-5 on non-NVIDIA chips, including Huawei Ascend, Moore Threads, Cambricon, Kunlun Chip, MetaX, Enflame, and Hygon. Through kernel optimization and model quantization, GLM-5 can achieve a reasonable throughput on those chips.\nFootnote\nHumanity’s Last Exam (HLE) & other reasoning tasks\n: We evaluate with a maximum generation length of 131,072 tokens (\ntemperature=1.0, top_p=0.95, max_new_tokens=131072\n). By default, we report the text-only subset; results marked with * are from the full set. We use GPT-5.2 (medium) as the judge model. For HLE-with-tools, we use a maximum context length of 202,752 tokens.\nSWE-bench & SWE-bench Multilingual\n: We run the SWE-bench suite with OpenHands using a tailored instruction prompt. Settings:\ntemperature=0.7, top_p=0.95, max_new_tokens=16384\n, with a 200K context window.\nBrowserComp\n: Without context management, we retain details from the most recent 5 turns. With context management, we use the same discard-all strategy as DeepSeek-V3.2 and Kimi K2.5.\nTerminal-Bench 2.0 (Terminus 2)\n: We evaluate with the Terminus framework using\ntimeout=2h, temperature=0.7, top_p=1.0, max_new_tokens=8192\n, with a 128K context window. Resource limits are capped at 16 CPUs and 32 GB RAM.\nCyberGym\n: We evaluate in Claude Code 2.1.18 (think mode, no web tools) with (\ntemperature=1.0, top_p=1.0, max_new_tokens=32000\n) and a 250-minute timeout per task. Results are single-run Pass@1 over 1,507 tasks.\nMCP-Atlas\n: All models are evaluated in think mode on the 500-task public subset with a 10-minute timeout per task. We use Gemini 3 Pro as the judge model.\nτ²-bench\n: We add a small prompt adjustment in Retail and Telecom to avoid failures caused by premature user termination. For Airline, we apply the domain fixes proposed in the Claude Opus 4.5 system card.\nVending Bench 2\n: Runs are conducted independently by\nAndon Labs\n.",
    "description": "",
    "url": "https://z.ai/blog/glm-5"
  },
  "cached_at": "2026-02-12T05:02:08.553347"
}
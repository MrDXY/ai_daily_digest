{
  "url": "https://mrshu.github.io/github-statuses/",
  "payload": {
    "title": "The Missing GitHub Status Page",
    "content": "Feb 9, 2026\n4:29 PM UTC • 1048 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nCopilot\nView updates\n4:29 PM UTC · Investigating — We are investigating reports of degraded performance for Copilot\n4:30 PM UTC · Update — We’ve identified an issue where Copilot policy updates are not propagating correctly for some customers. This may prevent newly enabled models from appearing when users try to access them. The team is actively investigating the cause and working on a resolution. We will provide updates as they become available.\n5:24 PM UTC · Update — We're continuing to investigate a an issue where Copilot policy updates are not propagating correctly for all customers. This may prevent newly enabled models from appearing when users try to access them.\n6:06 PM UTC · Update — We're continuing to investigate an issue where Copilot policy updates are not propagating correctly for a subset of enterprise users. This may prevent newly enabled models from appearing when users try to access them.\n6:49 PM UTC · Update — We're continuing to investigate an issue where Copilot policy updates are not propagating correctly for a subset of enterprise users. This may prevent newly enabled models from appearing when users try to access them. Next update in two hours.\n8:39 PM UTC · Update — We're continuing to investigate an issue where Copilot policy updates are not propagating correctly for a subset of enterprise users. This may prevent newly enabled models from appearing when users try to access them. Next update in two hours.\n10:09 PM UTC · Update — We're continuing to investigate an issue where Copilot policy updates are not propagating correctly for a subset of enterprise users. This may prevent newly enabled models from appearing when users try to access them. Next update in two hours.\n12:26 AM UTC · Update — We're continuing to address an issue where Copilot policy updates are not propagating correctly for a subset of enterprise users. This may prevent newly enabled models from appearing when users try to access them. This issue is understand and we are working to get the mitigation applied. Next update in one hour.\n12:51 AM UTC · Update — Copilot is operating normally.\n9:57 AM UTC · Resolved — This incident has been resolved.\n7:01 PM UTC • 68 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nResolved\nGit Operations\nWebhooks\nIssues\nPull Requests\n+4 more\nView updates\n7:01 PM UTC · Investigating — We are investigating reports of degraded performance for Actions, Git Operations and Issues\n7:02 PM UTC · Update — Actions is experiencing degraded availability. We are continuing to investigate.\n7:05 PM UTC · Update — Pages is experiencing degraded performance. We are continuing to investigate.\n7:07 PM UTC · Update — Pull Requests is experiencing degraded performance. We are continuing to investigate.\n7:07 PM UTC · Update — We are seeing impact to several systems including Actions, Copilot, Issues, and Git. Customers may see slow and failed requests, and Actions jobs being delayed. We are investigating.\n7:07 PM UTC · Update — Webhooks is experiencing degraded performance. We are continuing to investigate.\n7:10 PM UTC · Update — Packages is experiencing degraded performance. We are continuing to investigate.\n7:29 PM UTC · Update — We have applied mitigations and are seeing signs of recovery. We will continue to monitor for full recovery.\n7:31 PM UTC · Update — Codespaces is experiencing degraded performance. We are continuing to investigate.\n7:54 PM UTC · Update — A number of services have recovered, but we are continuing to investigate issues with Dependabot, Actions, and a number of other services. We will continue to investigate and monitor for full recovery.\n8:08 PM UTC · Update — We are seeing all services have returned to normal processing.\n8:09 PM UTC · Update — Actions, Codespaces, Git Operations, Issues, Packages, Pages, Pull Requests and Webhooks are operating normally.\n8:09 PM UTC · Resolved — On February 9, 2026, GitHub experienced two related periods of degraded availability affecting GitHub.com, the GitHub API, GitHub Actions, Git operations, GitHub Copilot, and other services. The first period occurred between 16:12 UTC and 17:39 UTC, and the second between 18:53 UTC and 20:09 UTC. In total, users experienced approximately 2 hours and 43 minutes of degraded service across the two incidents. During both incidents, users encountered errors loading pages on GitHub.com, failures when pushing or pulling code over HTTPS, failures starting or completing GitHub Actions workflow runs, and errors using GitHub Copilot. Additional services including GitHub Issues, pull requests, webhooks, Dependabot, GitHub Pages, and GitHub Codespaces experienced intermittent errors. SSH-based Git operations were not affected during either incident. Our investigation determined that both incidents shared the same underlying cause: a configuration change to a user settings caching mechanism caused a large volume of cache rewrites to occur simultaneously. During the first incident, asynchronous rewrites overwhelmed a shared infrastructure component responsible for coordinating background work, triggering cascading failures. Increased load caused the service responsible for proxying Git operations over HTTPS to exhaust available connections, preventing it from accepting new requests. We mitigated this incident by disabling async cache rewrites and restarting the affected Git proxy service across multiple datacenters. An additional source of updates to the same cache circumvented our initial mitigations and caused the second incident. This generated a high volume of synchronous writes, causing replication delays that cascaded in a similar pattern and again exhausted the Git proxy’s connection capacity, degrading availability across multiple services. We mitigated by disabling the source of the cache rewrites and again restarting Git proxy. We know these incidents disrupted the workflows of millions of developers. While we have made substantial, long-term investments in how GitHub is built and operated to improve resilience, GitHub's availability is not yet meeting our expectations. Getting there requires deep architectural work that is already underway, as well as urgent, targeted improvements. We are taking the following immediate steps: 1. We have already optimized the caching mechanism to avoid write amplification and added self-throttling during bulk updates. 2. We are adding safeguards to ensure the caching mechanism responds more quickly to rollbacks and strengthening how changes to these caching systems are planned, validated, and rolled out with additional checks. 3. We are fixing the underlying cause of connection exhaustion in our Git HTTPS proxy layer so the proxy can recover from this failure mode automatically without requiring manual restarts. GitHub is critical infrastructure for your work, your teams, and your businesses. We're focusing on these mitigations and long-term infrastructure work so GitHub is available, at scale, when and where you need it.\n8:09 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n3:54 PM UTC • 215 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nView updates\n3:54 PM UTC · Investigating — We are investigating reports of impacted performance for some GitHub services.\n4:12 PM UTC · Update — We are investigating notification delivery delays with the current delay being around 50 minutes. We are working on mitigation.\n4:51 PM UTC · Update — We continue to investigate delays in notification delivery with average delivery latency now nearing 1 hour 20 minutes. We are just now starting to see some signs of recovery.\n5:25 PM UTC · Update — We are seeing recovery in notification delivery. Notifications are currently being delivered with an average delay of approximately 1 hour as we work through the backlog. We continue to monitor the situation closely.\n5:57 PM UTC · Update — We are continuing to recover from notification delivery delays. Notifications are currently being delivered with an average delay of approximately 30 minutes. We are working through the remaining backlog.\n6:33 PM UTC · Update — We are continuing to recover from notification delivery delays. Notifications are currently being delivered with an average delay of approximately 15 minutes. We are working through the remaining backlog.\n7:14 PM UTC · Update — We continue observing recovery of the notifications. Notification delivery delays have been resolved.\n7:29 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n4:19 PM UTC • 81 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nResolved\nGit Operations\nWebhooks\nIssues\nPull Requests\n+2 more\nView updates\n4:19 PM UTC · Investigating — We are investigating reports of degraded performance for Pull Requests\n4:20 PM UTC · Update — Issues is experiencing degraded availability. We are continuing to investigate.\n4:21 PM UTC · Update — We are seeing intermittent errors on many pages and API requests and are investigating.\n4:22 PM UTC · Update — Actions is experiencing degraded performance. We are continuing to investigate.\n4:40 PM UTC · Update — Git Operations is experiencing degraded performance. We are continuing to investigate.\n4:40 PM UTC · Update — Webhooks is experiencing degraded performance. We are continuing to investigate.\n4:50 PM UTC · Update — Issues is experiencing degraded performance. We are continuing to investigate.\n4:58 PM UTC · Update — We have identified the cause of high error rates and taken steps to mitigate. We see early signs of recovery but are continuing to monitor impact.\n5:08 PM UTC · Update — Pages is experiencing degraded performance. We are continuing to investigate.\n5:25 PM UTC · Update — Issues is operating normally.\n5:26 PM UTC · Update — Git Operations is operating normally.\n5:29 PM UTC · Update — Pages is operating normally.\n5:32 PM UTC · Update — We are seeing recovery across all products and are continuing to monitor service health.\n5:37 PM UTC · Update — Actions is operating normally.\n5:39 PM UTC · Update — Webhooks is operating normally.\n5:40 PM UTC · Update — Pull Requests is operating normally.\n5:40 PM UTC · Resolved — On February 9, 2026, GitHub experienced two related periods of degraded availability affecting GitHub.com, the GitHub API, GitHub Actions, Git operations, GitHub Copilot, and other services. The first period occurred between 16:12 UTC and 17:39 UTC, and the second between 18:53 UTC and 20:09 UTC. In total, users experienced approximately 2 hours and 43 minutes of degraded service across the two incidents. During both incidents, users encountered errors loading pages on GitHub.com, failures when pushing or pulling code over HTTPS, failures starting or completing GitHub Actions workflow runs, and errors using GitHub Copilot. Additional services including GitHub Issues, pull requests, webhooks, Dependabot, GitHub Pages, and GitHub Codespaces experienced intermittent errors. SSH-based Git operations were not affected during either incident. Our investigation determined that both incidents shared the same underlying cause: a configuration change to a user settings caching mechanism caused a large volume of cache rewrites to occur simultaneously. During the first incident, asynchronous rewrites overwhelmed a shared infrastructure component responsible for coordinating background work, triggering cascading failures. Increased load caused the service responsible for proxying Git operations over HTTPS to exhaust available connections, preventing it from accepting new requests. We mitigated this incident by disabling async cache rewrites and restarting the affected Git proxy service across multiple datacenters. An additional source of updates to the same cache circumvented our initial mitigations and caused the second incident. This generated a high volume of synchronous writes, causing replication delays that cascaded in a similar pattern and again exhausted the Git proxy’s connection capacity, degrading availability across multiple services. We mitigated by disabling the source of the cache rewrites and again restarting Git proxy. We know these incidents disrupted the workflows of millions of developers. While we have made substantial, long-term investments in how GitHub is built and operated to improve resilience, GitHub's availability is not yet meeting our expectations. Getting there requires deep architectural work that is already underway, as well as urgent, targeted improvements. We are taking the following immediate steps: 1. We have already optimized the caching mechanism to avoid write amplification and added self-throttling during bulk updates. 2. We are adding safeguards to ensure the caching mechanism responds more quickly to rollbacks and strengthening how changes to these caching systems are planned, validated, and rolled out with additional checks. 3. We are fixing the underlying cause of connection exhaustion in our Git HTTPS proxy layer so the proxy can recover from this failure mode automatically without requiring manual restarts. GitHub is critical infrastructure for your work, your teams, and your businesses. We're focusing on these mitigations and long-term infrastructure work so GitHub is available, at scale, when and where you need it.\n5:40 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n2:17 PM UTC • 89 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nActions\nView updates\n2:17 PM UTC · Investigating — We are investigating reports of degraded performance for Actions\n2:17 PM UTC · Update — We are investigating an issue with Actions run start delays, impacting approximately 4% of users.\n2:54 PM UTC · Update — We continue to investigate an issue causing Actions run start delays, impacting approximately 4% of users.\n3:26 PM UTC · Update — We identified a bottleneck in our processing pipeline and have applied mitigations. We will continue to monitor for full recovery.\n3:46 PM UTC · Update — Actions is operating normally.\n3:46 PM UTC · Update — Actions run delays have returned to normal levels.\n3:46 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n10:01 AM UTC • 131 min impact\nInvestigating\nUpdate\nUpdate\nResolved\nCopilot\nView updates\n10:01 AM UTC · Investigating — We are investigating reports of impacted performance for some GitHub services.\n10:04 AM UTC · Update — We are investigating degraded availability for Copilot Coding Agent. We will continue to keep users updated on progress towards mitigation.\n11:14 AM UTC · Update — We are continuing to investigate the degraded availability for Copilot Coding Agent.\n12:12 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n8:15 AM UTC • 191 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nGit Operations\nWebhooks\nIssues\nPull Requests\n+1 more\nView updates\n8:15 AM UTC · Investigating — We are investigating reports of degraded performance for Pull Requests and Webhooks\n8:17 AM UTC · Update — Issues is experiencing degraded performance. We are continuing to investigate.\n8:17 AM UTC · Update — We are investigating intermittent latency and errors with Webhooks API, Webhooks UI, and PRs. We will continue to keep users updated on progress towards mitigation.\n8:52 AM UTC · Update — We are continuing to investigate intermittent elevated timeouts.\n9:23 AM UTC · Update — Actions is experiencing degraded performance. We are continuing to investigate.\n9:31 AM UTC · Update — We are continuing to investigate intermittent elevated timeouts across the service. Current impact is estimated around 1% or less of requests.\n10:09 AM UTC · Update — We are continuing to investigate intermittent elevated timeouts across the service.\n10:33 AM UTC · Update — Git Operations is experiencing degraded performance. We are continuing to investigate.\n10:48 AM UTC · Update — We are continuing to investigate intermittent elevated timeouts across the service.\n11:04 AM UTC · Update — Git Operations is operating normally.\n11:11 AM UTC · Update — We have identified a faulty infrastructure component and have failed over to a healthy instance. We are continuing to monitor the system for recovery.\n11:26 AM UTC · Update — Actions is operating normally.\n11:26 AM UTC · Update — Issues is operating normally.\n11:26 AM UTC · Update — Pull Requests is operating normally.\n11:26 AM UTC · Update — Webhooks is operating normally.\n11:26 AM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\nFeb 2, 2026\n6:35 PM UTC • 220 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nResolved\nActions\nPages\nCopilot\nView updates\n7:03 PM UTC · Investigating — We are investigating reports of degraded performance for Actions\n7:07 PM UTC · Update — GitHub Actions hosted runners are experiencing high wait times across all labels. Self-hosted runners are not impacted.\n7:43 PM UTC · Update — Actions is experiencing degraded availability. We are continuing to investigate.\n7:44 PM UTC · Update — The team continues to investigate issues causing GitHub Actions jobs on hosted runners to remain queued for extended periods, with a percentage of jobs failing. We will continue to provide updates as we make progress toward mitigation.\n7:48 PM UTC · Update — Pages is experiencing degraded performance. We are continuing to investigate.\n8:27 PM UTC · Update — The team continues to investigate issues causing GitHub Actions jobs on hosted runners to remain queued for extended periods, with a percentage of jobs failing. We will continue to provide updates as we make progress toward mitigation.\n9:13 PM UTC · Update — We continue to investigate failures impacting GitHub Actions hosted-runner jobs. We have identified the root cause and are working with our upstream provider to mitigate. This is also impacting GitHub features that rely on GitHub Actions (for example, Copilot Coding Agent and Dependabot).\n9:27 PM UTC · Update — Copilot is experiencing degraded performance. We are continuing to investigate.\n10:10 PM UTC · Update — We continue to investigate failures impacting GitHub Actions hosted-runner jobs. We're waiting on our upstream provider to apply the identified mitigations, and we're preparing to resume job processing as safely as possible.\n10:53 PM UTC · Update — Our upstream provider has applied a mitigation to address queuing and job failures on hosted runners. Telemetry shows improvement, and we are monitoring closely for full recovery.\n11:31 PM UTC · Update — Pages is operating normally.\n11:42 PM UTC · Update — Copilot is operating normally.\n11:43 PM UTC · Update — Actions is experiencing degraded performance. We are continuing to investigate.\n11:50 PM UTC · Update — Based on our telemetry, most customers should see full recovery from failing GitHub Actions jobs on hosted runners. We are monitoring closely to confirm complete recovery. Other GitHub features that rely on GitHub Actions (for example, Copilot Coding Agent and Dependabot) should also see recovery.\n12:56 AM UTC · Update — Actions is operating normally.\n12:56 AM UTC · Resolved — On February 2, 2026, between 18:35 UTC and 22:15 UTC, GitHub Actions hosted runners were unavailable, with service degraded until full recovery at 23:10 UTC for standard runners and at February 3, 2026 00:30 UTC for larger runners. During this time, Actions jobs queued and timed out while waiting to acquire a hosted runner. Other GitHub features that leverage this compute infrastructure were similarly impacted, including Copilot Coding Agent, Copilot Code Review, CodeQL, Dependabot, GitHub Enterprise Importer, and Pages. All regions and runner types were impacted. Self-hosted runners on other providers were not impacted. This outage was caused by a backend storage access policy change in our underlying compute provider that blocked access to critical VM metadata, causing all VM create, delete, reimage, and other operations to fail. More information is available at https://azure.status.microsoft/en-us/status/history/?trackingId=FNJ8-VQZ. This was mitigated by rolling back the policy change, which started at 22:15 UTC. As VMs came back online, our runners worked through the backlog of requests that hadn’t timed out. We are working with our compute provider to improve our incident response and engagement time, improve early detection before they impact our customers, and ensure safe rollout should similar changes occur in the future. We recognize this was a significant outage to our users that rely on GitHub’s workloads and apologize for the impact this had.\n12:56 AM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n8:17 PM UTC • 277 min impact\nInvestigating\nUpdate\nUpdate\nUpdate\nUpdate\nResolved\nResolved\nCodespaces\nView updates\n8:17 PM UTC · Investigating — We are investigating reports of degraded availability for Codespaces\n8:19 PM UTC · Update — Users may see errors creating or resuming codespaces. We are investigating and will provide further updates as we have them.\n11:52 PM UTC · Update — Codespaces is seeing steady recovery\n12:25 AM UTC · Update — Codespaces is experiencing degraded performance. We are continuing to investigate.\n12:54 AM UTC · Update — Codespaces is operating normally.\n12:54 AM UTC · Resolved — On February 2, 2026, GitHub Codespaces were unavailable between 18:55 and 22:20 UTC and degraded until the service fully recovered at February 3, 2026 00:15 UTC. During this time, Codespaces creation and resume operations failed in all regions. This outage was caused by a backend storage access policy change in our underlying compute provider that blocked access to critical VM metadata, causing all VM create, delete, reimage, and other operations to fail. More information is available at https://azure.status.microsoft/en-us/status/history/?trackingId=FNJ8-VQZ. This was mitigated by rolling back the policy change, which started at 22:15 UTC. As VMs came back online, our runners worked through the backlog of requests that hadn’t timed out. We are working with our compute provider to improve our incident response and engagement time, improve early detection before they impact our customers, and ensure safe rollout should similar changes occur in the future. We recognize this was a significant outage to our users that rely on GitHub’s workloads and apologize for the impact this had.\n12:54 AM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n5:41 PM UTC • 65 min impact\nInvestigating\nUpdate\nResolved\nResolved\nPull Requests\nView updates\n5:41 PM UTC · Investigating — We are investigating reports of impacted performance for some GitHub services.\n5:58 PM UTC · Update — Dependabot is currently experiencing an issue that may cause scheduled update jobs to fail when creating pull requests. Our team has identified the problem and deployed a fix. We’re seeing signs of recovery and expect full resolution within the next few hours.\n6:46 PM UTC · Resolved — From Jan 31, 2026 00:30 UTC to Feb 2, 2026 18:00 UTC Dependabot service was degraded and failed to create 10% of Automated Pull Requests. This was due to a cluster failover that connected to a read-only database. We mitigated the incident by pausing Dependabot queues until traffic was properly routed to healthy clusters. We’re working on identifying and rerunning all failed jobs during this time. We’re adding new monitors and alerts to reduce our time to detection and prevent this in the future.\n6:46 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.\n5:34 PM UTC • 9 min impact\nInvestigating\nUpdate\nUpdate\nResolved\nResolved\nGit Operations\nView updates\n5:34 PM UTC · Investigating — We are investigating reports of impacted performance for some GitHub services.\n5:35 PM UTC · Update — Git Operations is experiencing degraded performance. We are continuing to investigate.\n5:42 PM UTC · Update — We’ve observed a low rate (~0.01%) of 5xx errors for HTTP-based fetches and clones. We’re currently routing traffic away from the affected location and are seeing recovery.\n5:43 PM UTC · Resolved — From Feb 2, 2026 17:13 UTC to Feb 2, 2026 17:36 UTC we experienced failures on ~0.02% of Git operations. While deploying an internal service, a misconfiguration caused a small subset of traffic to route to a service that was not ready. During the incident we observed the degradation and statused publicly. To mitigate the issue, traffic was redirected to healthy instances and we resumed normal operation. We are improving our monitoring and deployment processes in this area to avoid future routing issues.\n5:43 PM UTC · Resolved — This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.",
    "description": "",
    "id": null,
    "short_id": null,
    "url": "https://mrshu.github.io/github-statuses/",
    "score": "130",
    "tags": "vcs",
    "domain": "mrshu.github.io",
    "author": "skade",
    "author_url": "skade",
    "time": "33 hours ago",
    "comments_text": "21 comments",
    "comments_url": "21 comments",
    "description_hint": null
  },
  "cached_at": "2026-02-12T05:05:35.752342"
}
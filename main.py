#!/usr/bin/env python3
"""
AI å†…å®¹è„±æ°´æ—¥æŠ¥ - ä¸»å…¥å£

Usage:
    python main.py                    # ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
    python main.py --config path.yaml # ä½¿ç”¨æŒ‡å®šé…ç½®
    python main.py --provider claude  # æŒ‡å®š AI provider
    python main.py --dry-run          # è¯•è¿è¡Œï¼ˆä¸è°ƒç”¨ AIï¼‰
"""

import argparse
import asyncio
import logging
import sys
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

from notifier.readme_updater import update_readme, ReadmeUpdater

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.core.config import load_config, AppConfig, get_report_date_dir, get_cache_dir
from src.core.models import FetchTask, FetchResult, FetchStatus, Article, DigestReport
from src.core.exceptions import DigestException
from src.crawler import FetchManager
from src.processor import ProcessingPipeline, HTMLCleaner
from src.notifier import ReportGenerator, TerminalDisplay
from src.generator import ConfigGenerator


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-7s | %(name)s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("main")


class DailyDigestApp:
    """
    æ—¥æŠ¥ç”Ÿæˆåº”ç”¨

    ç¼–æ’æ•´ä¸ªå¤„ç†æµç¨‹ï¼š
    1. åŠ è½½é…ç½®
    2. åˆ›å»ºæŠ“å–ä»»åŠ¡
    3. æ‰§è¡ŒæŠ“å–
    4. å¤„ç†å’Œæ‘˜è¦
    5. ç”ŸæˆæŠ¥å‘Š
    """

    def __init__(
        self,
        config_path: Optional[str] = None,
        provider: Optional[str] = None,
        dry_run: bool = False,
    ):
        self.config_path = config_path
        self.provider_override = provider
        self.dry_run = dry_run

        self.config: Optional[AppConfig] = None
        self.display = TerminalDisplay()
        self.updater = ReadmeUpdater(readme_path="README.md", report_dir="output/report")

    async def run(self) -> DigestReport:
        """è¿è¡Œå®Œæ•´çš„æ—¥æŠ¥ç”Ÿæˆæµç¨‹"""
        start_time = time.time()

        # æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…
        self.display.show_banner()

        # åŠ è½½é…ç½®
        self.config = load_config(self.config_path)

        # é‡æ–°åˆå§‹åŒ–æ˜¾ç¤ºå™¨ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„è¯„åˆ†é˜ˆå€¼ï¼‰
        self.display = TerminalDisplay(
            score_threshold=float(self.config.digest.score_threshold),
            show_low_score=self.config.output.terminal.get("show_low_score", False),
        )

        # è¦†ç›– AI provider
        if self.provider_override:
            self.config.ai.default_provider = self.provider_override

        # æ˜¾ç¤ºé…ç½®
        self._show_config_summary()

        # åˆ›å»ºä»»åŠ¡
        tasks = self._create_fetch_tasks()
        logger.info(f"Created {len(tasks)} fetch tasks")

        if not tasks:
            self.display.show_warning("No tasks to process")
            return DigestReport(date=datetime.now().strftime("%Y-%m-%d"))

        # ä½¿ç”¨ FetchManager è¿›è¡ŒæŠ“å–å’Œå¤„ç†ï¼ˆä¿æŒ manager å¯ç”¨äºäºŒæ¬¡çˆ¬å–ï¼‰
        errors: list[str] = []

        async with FetchManager(
            self.config.crawler,
        ) as fetch_manager:
            # æ‰§è¡ŒæŠ“å–
            self.display.show_info("Starting fetch...")
            results = await self._execute_fetch_with_manager(tasks, fetch_manager)

            # ç»Ÿè®¡æŠ“å–ç»“æœ
            success_count = sum(1 for r in results if r.status == FetchStatus.SUCCESS)
            failed_count = len(results) - success_count
            self.display.show_fetch_result(success_count, failed_count, len(results))

            # è®°å½•æŠ“å–å¤±è´¥
            errors.extend(self._collect_fetch_errors(results))

            # å¤„ç†å’Œæ‘˜è¦
            if self.dry_run:
                self.display.show_info("Dry run mode - skipping AI processing")
                articles = self._create_mock_articles(results)
            else:
                self.display.show_info("Processing and summarizing...")
                articles, pipeline_errors = await self._process_results(results, fetch_manager)
                errors.extend(pipeline_errors)

        # ç”ŸæˆæŠ¥å‘Š
        self.display.show_info("Generating report...")
        processing_time = time.time() - start_time

        generator = ReportGenerator(self.config)
        report = await generator.generate(
            articles=articles,
            total_fetched=len(tasks),
            errors=errors,
            processing_time=processing_time,
        )

        # æ˜¾ç¤ºé«˜è´¨é‡é¡¹ç›®ï¼ˆä½¿ç”¨å»é‡åçš„æŠ¥å‘Šç»“æœï¼‰
        self.display.show_high_quality_articles(report.articles)

        # æ˜¾ç¤ºæ‘˜è¦
        self.display.show_report_summary(report)

        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        report_dir = get_report_date_dir(self.config, report.date)
        report_path = report_dir / self.config.output.report_filename.format(
            date=report.date
        )
        self.display.show_completion(str(report_path))

        print("æ­£åœ¨æ›´æ–° README ç´¢å¼•...")
        self.updater.update()

        return report

    def _show_config_summary(self) -> None:
        """æ˜¾ç¤ºé…ç½®æ‘˜è¦"""
        enabled_sites = [s.name for s in self.config.sites if s.enabled]

        summary = {
            "AI Provider": self.config.ai.default_provider,
            "Sites": ", ".join(enabled_sites),
            "Concurrency": self.config.crawler.concurrency,
            "Score Threshold": self.config.digest.score_threshold,
            "Dry Run": self.dry_run,
        }

        self.display.show_config(summary)

    def _create_fetch_tasks(self) -> list[FetchTask]:
        """åˆ›å»ºæŠ“å–ä»»åŠ¡"""
        tasks = []

        for site_ref in self.config.sites:
            if not site_ref.enabled:
                continue

            site_config = self.config.site_configs.get(site_ref.name, {})
            if not site_config:
                logger.warning(f"No config found for site: {site_ref.name}")
                continue

            site_url = site_config.get("site", {}).get("url")
            if not site_url:
                logger.warning(f"No URL found for site: {site_ref.name}")
                continue

            task = FetchTask(
                id=f"{site_ref.name}_{uuid.uuid4().hex[:8]}",
                url=site_url,
                site_name=site_ref.name,
                site_config=site_config,
            )
            tasks.append(task)

        return tasks

    async def _execute_fetch_with_manager(
        self,
        tasks: list[FetchTask],
        manager: 'FetchManager',
    ) -> list[FetchResult]:
        """ä½¿ç”¨æŒ‡å®šçš„ FetchManager æ‰§è¡ŒæŠ“å–"""
        # ä½¿ç”¨è¿›åº¦æ¡
        progress = self.display.create_progress()

        with progress:
            task_id = progress.add_task(
                "Fetching...",
                total=len(tasks),
            )

            results = []
            for task in tasks:
                result = await manager.fetch(task)
                results.append(result)
                progress.update(task_id, advance=1)

        return results

    async def _execute_fetch(self, tasks: list[FetchTask]) -> list[FetchResult]:
        """æ‰§è¡ŒæŠ“å–ï¼ˆç‹¬ç«‹ä½¿ç”¨ FetchManagerï¼‰"""
        # è·å–ç¼“å­˜é…ç½®
        cache_dir = get_cache_dir(self.config)
        cache_enabled = self.config.crawler.cache.enabled

        async with FetchManager(
            self.config.crawler,
            cache_dir=cache_dir,
            cache_enabled=cache_enabled,
        ) as manager:
            # æ¸…ç†æ—§ç¼“å­˜
            keep_days = self.config.crawler.cache.keep_days
            manager.clear_old_cache(keep_days)

            results = await self._execute_fetch_with_manager(tasks, manager)

            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            cache_stats = manager.get_stats().get("cache", {})
            if cache_stats.get("enabled"):
                self.display.show_info(
                    f"Cache: {cache_stats.get('total_files', 0)} files, "
                    f"{cache_stats.get('total_size_mb', 0)} MB"
                )

        return results

    async def _process_results(
        self,
        results: list[FetchResult],
        fetch_manager: Optional['FetchManager'] = None,
    ) -> tuple[list[Article], list[str]]:
        """å¤„ç†æŠ“å–ç»“æœ"""
        articles = []
        errors: list[str] = []

        async with ProcessingPipeline(self.config) as pipeline:
            # è®¾ç½®äºŒæ¬¡çˆ¬å–å›è°ƒï¼ˆå¦‚æœæœ‰ fetch_managerï¼‰
            if fetch_manager:
                async def fetch_details_callback(urls: list[str], site_config: dict) -> list[FetchResult]:
                    """äºŒæ¬¡çˆ¬å–è¯¦æƒ…é¡µçš„å›è°ƒå‡½æ•°"""
                    detail_tasks = []
                    for url in urls:
                        task = FetchTask(
                            id=f"detail_{uuid.uuid4().hex[:8]}",
                            url=url,
                            site_name="detail",
                            site_config=site_config,
                        )
                        detail_tasks.append(task)

                    return await fetch_manager.fetch_many(detail_tasks)

                pipeline.set_fetch_callback(fetch_details_callback)

            progress = self.display.create_progress()

            with progress:
                task_id = progress.add_task(
                    "Processing...",
                    total=len(results),
                )

                for result in results:
                    if result.status != FetchStatus.SUCCESS:
                        progress.update(task_id, advance=1)
                        continue

                    # æŸ¥æ‰¾å¯¹åº”çš„ç«™ç‚¹é…ç½®
                    site_config = {}
                    for site_ref in self.config.sites:
                        if site_ref.name in result.task_id:
                            site_config = self.config.site_configs.get(
                                site_ref.name, {}
                            )
                            break

                    try:
                        site_articles = await pipeline.process(result, site_config)
                        articles.extend(site_articles)
                    except Exception as e:
                        logger.error(f"Processing error: {e}")

                    progress.update(task_id, advance=1)

            errors.extend(pipeline.get_errors())

        return articles, errors

    def _collect_fetch_errors(self, results: list[FetchResult]) -> list[str]:
        """æ”¶é›†æŠ“å–å¤±è´¥é”™è¯¯ä¿¡æ¯"""
        errors: list[str] = []
        for result in results:
            if result.status == FetchStatus.SUCCESS:
                continue

            status_code = result.status_code or "N/A"
            error_message = result.error_message or "Unknown error"
            errors.append(
                f"æŠ“å–å¤±è´¥: {result.url} | HTTP {status_code} | {error_message}"
            )

        return errors

    def _create_mock_articles(
        self,
        results: list[FetchResult],
    ) -> list[Article]:
        """åˆ›å»ºæ¨¡æ‹Ÿæ–‡ç« ï¼ˆç”¨äº dry-run æ¨¡å¼ï¼‰"""
        articles = []
        cleaner = HTMLCleaner()

        for result in results:
            if result.status != FetchStatus.SUCCESS or not result.html:
                continue

            # æŸ¥æ‰¾ç«™ç‚¹é…ç½®
            site_config = {}
            site_name = "Unknown"
            for site_ref in self.config.sites:
                if site_ref.name in result.task_id:
                    site_config = self.config.site_configs.get(site_ref.name, {})
                    site_name = site_config.get("site", {}).get("name", site_ref.name)
                    break

            # æå–åŸºæœ¬ä¿¡æ¯
            parser_config = site_config.get("list_parser", {})
            container = parser_config.get("container")
            selectors = parser_config.get("selectors", {})
            url_prefix = parser_config.get("url_prefix", "")

            if container and selectors:
                # è¿‡æ»¤æ‰æ— æ•ˆçš„ CSS é€‰æ‹©å™¨ï¼ˆå¦‚ç›¸é‚»å…„å¼Ÿé€‰æ‹©å™¨ "+ tr"ï¼‰
                valid_selectors = {
                    k: v for k, v in selectors.items()
                    if v and not v.strip().startswith('+')
                }
                items = cleaner.extract_structured(
                    html=result.html,
                    selectors=valid_selectors,
                    container_selector=container,
                    base_url=result.url,
                    url_prefix=url_prefix,
                )

                for i, item in enumerate(items[:10]):  # é™åˆ¶æ•°é‡
                    article = Article(
                        id=f"mock_{uuid.uuid4().hex[:8]}",
                        source=site_name,
                        url=item.get("url", result.url),
                        title=item.get("title", f"Mock Article {i+1}"),
                        description=item.get("description"),
                        summary="[Dry Run] è¿™æ˜¯æ¨¡æ‹Ÿæ‘˜è¦ï¼Œå®é™…è¿è¡Œæ—¶ä¼šè°ƒç”¨ AI ç”Ÿæˆã€‚",
                        core_value="[Dry Run] æ¨¡æ‹Ÿæ ¸å¿ƒä»·å€¼",
                        tech_stack=["Python", "Mock"],
                        recommendation="[Dry Run] æ¨¡æ‹Ÿæ¨èç†ç”±",
                        score=50.0 + (i % 5) * 10,  # æ¨¡æ‹Ÿ 50-90 åˆ†
                        language=item.get("language"),
                    )
                    articles.append(article)

        return articles


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="AI å†…å®¹è„±æ°´æ—¥æŠ¥ç”Ÿæˆå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "-c", "--config",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/config.yaml)",
    )

    parser.add_argument(
        "-p", "--provider",
        type=str,
        choices=["claude", "openai", "azure_openai", "custom"],
        help="æŒ‡å®š AI provider",
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸è°ƒç”¨ AIï¼‰",
    )

    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡º",
    )

    # é…ç½®ç”Ÿæˆç›¸å…³å‚æ•°
    parser.add_argument(
        "--generate-config",
        type=str,
        metavar="URL",
        help="æ ¹æ®æŒ‡å®š URL ç”Ÿæˆç«™ç‚¹é…ç½®æ–‡ä»¶",
    )

    parser.add_argument(
        "--use-js",
        action="store_true",
        help="ç”Ÿæˆé…ç½®æ—¶ä½¿ç”¨ JS æ¸²æŸ“ï¼ˆé…åˆ --generate-config ä½¿ç”¨ï¼‰",
    )

    parser.add_argument(
        "--output",
        type=str,
        help="é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„ï¼ˆé…åˆ --generate-config ä½¿ç”¨ï¼‰",
    )

    return parser.parse_args()


async def main() -> int:
    """ä¸»å‡½æ•°"""
    args = parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # å¦‚æœæ˜¯ç”Ÿæˆé…ç½®æ¨¡å¼
        if args.generate_config:
            return await generate_site_config(
                url=args.generate_config,
                config_path=args.config,
                provider=args.provider,
                use_js=args.use_js,
                output_path=args.output,
            )

        # æ­£å¸¸çš„æ—¥æŠ¥ç”Ÿæˆæ¨¡å¼
        app = DailyDigestApp(
            config_path=args.config,
            provider=args.provider,
            dry_run=args.dry_run,
        )

        report = await app.run()

        return 0 if report.total_processed > 0 else 1

    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted by user")
        return 130

    except DigestException as e:
        logger.error(f"Application error: {e}")
        return 1

    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
        return 1


async def generate_site_config(
    url: str,
    config_path: Optional[str] = None,
    provider: Optional[str] = None,
    use_js: bool = False,
    output_path: Optional[str] = None,
) -> int:
    """
    ç”Ÿæˆç«™ç‚¹é…ç½®æ–‡ä»¶

    Args:
        url: ç›®æ ‡ URL
        config_path: ä¸»é…ç½®æ–‡ä»¶è·¯å¾„
        provider: AI provider
        use_js: æ˜¯å¦ä½¿ç”¨ JS æ¸²æŸ“
        output_path: è¾“å‡ºè·¯å¾„

    Returns:
        é€€å‡ºç 
    """
    from rich.console import Console
    from rich.panel import Panel
    from rich.syntax import Syntax

    console = Console()

    console.print(Panel.fit(
        f"[bold blue]ğŸ”§ ç«™ç‚¹é…ç½®ç”Ÿæˆå™¨[/bold blue]\n\n"
        f"ç›®æ ‡ URL: [cyan]{url}[/cyan]\n"
        f"JS æ¸²æŸ“: [yellow]{'æ˜¯' if use_js else 'å¦'}[/yellow]",
        border_style="blue",
    ))

    # åŠ è½½é…ç½®
    config = load_config(config_path)

    # è¦†ç›– AI provider
    if provider:
        config.ai.default_provider = provider

    console.print(f"\n[dim]AI Provider: {config.ai.default_provider}[/dim]\n")

    # åˆ›å»ºç”Ÿæˆå™¨
    generator = ConfigGenerator(config)

    try:
        with console.status("[bold green]æ­£åœ¨å¤„ç†...", spinner="dots"):
            config_file_path, config_content = await generator.generate_config(
                url=url,
                use_js=use_js,
                output_path=output_path,
            )

        # æ˜¾ç¤ºç”Ÿæˆçš„é…ç½®
        console.print("\n[bold green]âœ… é…ç½®ç”ŸæˆæˆåŠŸï¼[/bold green]\n")
        console.print(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: [cyan]{config_file_path}[/cyan]\n")

        # æ˜¾ç¤ºé…ç½®å†…å®¹é¢„è§ˆ
        console.print("[bold]ç”Ÿæˆçš„é…ç½®å†…å®¹:[/bold]\n")
        syntax = Syntax(config_content, "yaml", theme="monokai", line_numbers=True)
        console.print(syntax)

        # æç¤ºåç»­æ“ä½œ
        console.print("\n[dim]æç¤º: è¯·æ£€æŸ¥ç”Ÿæˆçš„é…ç½®æ–‡ä»¶ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚[/dim]")
        console.print("[dim]ç„¶åå°†å…¶æ·»åŠ åˆ° config/config.yaml çš„ sites åˆ—è¡¨ä¸­å³å¯ä½¿ç”¨ã€‚[/dim]")

        return 0

    except Exception as e:
        console.print(f"\n[bold red]âŒ é…ç½®ç”Ÿæˆå¤±è´¥: {e}[/bold red]")
        logger.exception("é…ç½®ç”Ÿæˆé”™è¯¯")
        return 1

    finally:
        await generator.close()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

"""
æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆ Markdown å’Œ JSON æ ¼å¼çš„æ—¥æŠ¥
"""

import json
import logging
import re
from difflib import SequenceMatcher
from datetime import datetime
from pathlib import Path
from typing import Any, Optional
from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse

import aiofiles
from jinja2 import Environment, FileSystemLoader, select_autoescape

from ..core.models import Article, DigestReport
from ..core.config import AppConfig, get_output_dir


logger = logging.getLogger(__name__)


# Markdown æŠ¥å‘Šæ¨¡æ¿
REPORT_TEMPLATE = """# ğŸ—ï¸ AI å†…å®¹è„±æ°´æ—¥æŠ¥

ğŸ“… **æ—¥æœŸ**: {{ report.date }}
â±ï¸ **ç”Ÿæˆæ—¶é—´**: {{ report.generated_at.strftime('%Y-%m-%d %H:%M:%S') }}

---

## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| ğŸ“¥ æŠ“å–æ•°é‡ | {{ report.total_fetched }} |
| âœ… å¤„ç†æ•°é‡ | {{ report.total_processed }} |
| ğŸŒŸ é«˜è´¨é‡é¡¹ç›® | {{ report.high_quality_count }} |
| ğŸ“ˆ å¹³å‡è¯„åˆ† | {{ "%.1f"|format(report.avg_score) }} |

### æ¥æºåˆ†å¸ƒ
{% for source, count in report.sources.items() %}
- **{{ source }}**: {{ count }} ç¯‡
{% endfor %}

---

## ğŸŒŸ é«˜è´¨é‡é¡¹ç›® (è¯„åˆ† â‰¥ {{ score_threshold }})

{% for article in high_quality_articles %}
### {{ loop.index }}. [{{ article.title }}]({{ article.url }})

{% if article.stars %}â­ {{ article.stars }} stars {% endif %}{% if article.language %}| ğŸ”¤ {{ article.language }}{% endif %}

**è¯„åˆ†**: {{ "â­" * ((article.score / 10) | int) }} ({{ article.score }}/100)

**æ ¸å¿ƒä»·å€¼**: {{ article.core_value }}

**æŠ€æœ¯æ ˆ**: {{ article.tech_stack | join(", ") if article.tech_stack else "N/A" }}

**æ‘˜è¦**: {{ article.summary }}

**æ¨èç†ç”±**: {{ article.recommendation }}

---

{% endfor %}

{% if other_articles %}
## ğŸ“š å…¶ä»–é¡¹ç›®

{% for article in other_articles %}
### {{ loop.index }}. [{{ article.title }}]({{ article.url }}) - {{ article.score }}/100

{{ article.summary }}

---

{% endfor %}
{% endif %}

---

## ğŸ“ å¤„ç†æ—¥å¿—

{% if report.errors %}
### âš ï¸ é”™è¯¯è®°å½•
{% for error in report.errors %}
- {{ error }}
{% endfor %}
{% else %}
âœ… æœ¬æ¬¡å¤„ç†æ— é”™è¯¯
{% endif %}

---

> ğŸ¤– ç”± AI Daily Digest è‡ªåŠ¨ç”Ÿæˆ
> 
> å¤„ç†è€—æ—¶: {{ "%.2f"|format(report.processing_time_seconds) }} ç§’
"""


class ReportGenerator:
    """
    æŠ¥å‘Šç”Ÿæˆå™¨

    æ”¯æŒç”Ÿæˆï¼š
    - Markdown æ ¼å¼æŠ¥å‘Š
    - JSON æ ¼å¼æ•°æ®
    - è‡ªå®šä¹‰æ¨¡æ¿
    """

    def __init__(self, config: AppConfig):
        self.config = config
        self.output_dir = get_output_dir(config)

        # åˆå§‹åŒ– Jinja2 ç¯å¢ƒ
        template_dir = Path(__file__).parent.parent.parent / "templates"
        if template_dir.exists():
            self.env = Environment(
                loader=FileSystemLoader(str(template_dir)),
                autoescape=select_autoescape(["html", "xml"]),
            )
        else:
            self.env = Environment(autoescape=select_autoescape(["html", "xml"]))

    async def generate(
        self,
        articles: list[Article],
        total_fetched: int = 0,
        errors: list[Any] = None,
        processing_time: float = 0.0,
    ) -> DigestReport:
        """
        ç”ŸæˆæŠ¥å‘Š

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            total_fetched: æ€»æŠ“å–æ•°
            errors: é”™è¯¯åˆ—è¡¨
            processing_time: å¤„ç†è€—æ—¶

        Returns:
            DigestReport å¯¹è±¡
        """
        # å…ˆå»é‡ï¼Œå†æ’åºï¼ˆä»·å€¼é«˜åˆ°ä½ï¼Œå—ä¼—å°çš„é åï¼‰
        deduped_articles = self._deduplicate_articles(articles)
        sorted_articles = self._sort_articles(deduped_articles)

        # åˆ›å»ºæŠ¥å‘Šå¯¹è±¡
        report = DigestReport(
            date=datetime.now().strftime("%Y-%m-%d"),
            total_fetched=total_fetched,
            articles=sorted_articles,
            errors=errors or [],
            processing_time_seconds=processing_time,
            score_threshold=float(self.config.digest.score_threshold),
        )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        report.calculate_stats()

        # ç”Ÿæˆæ–‡ä»¶
        md_path = await self._generate_markdown(report)

        if self.config.output.generate_json:
            await self._generate_json(report)

        return report

    async def _generate_markdown(self, report: DigestReport) -> Path:
        """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
        # ä½¿ç”¨å†…ç½®æ¨¡æ¿
        from jinja2 import Template
        template = Template(REPORT_TEMPLATE)

        # åˆ†ç¦»é«˜è´¨é‡å’Œå…¶ä»–æ–‡ç« 
        high_quality = report.get_high_quality_articles()
        other = [a for a in report.articles if not a.is_high_quality]

        # æ¸²æŸ“æ¨¡æ¿
        content = template.render(
            report=report,
            high_quality_articles=high_quality,
            other_articles=other,
            score_threshold=report.score_threshold,
        )

        # ç”Ÿæˆæ–‡ä»¶å
        filename = self.config.output.report_filename.format(
            date=report.date
        )
        filepath = self.output_dir / filename

        # å†™å…¥æ–‡ä»¶
        async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
            await f.write(content)

        return filepath

    def _sort_articles(self, articles: list[Article]) -> list[Article]:
        """æŒ‰ä»·å€¼ä¼˜å…ˆï¼Œå†æŒ‰å—ä¼—è§„æ¨¡æ’åº"""
        return sorted(
            articles,
            key=lambda a: (
                -(a.score or 0),
                -self._get_audience_size(a),
                a.title or "",
            ),
        )

    def _deduplicate_articles(self, articles: list[Article]) -> list[Article]:
        """åŸºäºæ ‡é¢˜ä¸æ‘˜è¦ç›¸ä¼¼åº¦å»é‡ï¼ˆè·¨ç«™ç‚¹ï¼‰"""
        if not articles:
            return []

        # å…ˆæŒ‰è¯„åˆ†æ’åºï¼Œç¡®ä¿ä¿ç•™è´¨é‡æ›´é«˜çš„ç‰ˆæœ¬
        candidates = sorted(
            articles,
            key=lambda a: (-(a.score or 0), -(len(a.summary or "")), a.title or ""),
        )

        kept: list[Article] = []
        kept_cache: list[dict[str, Any]] = []
        seen_urls: set[str] = set()

        for article in candidates:
            canonical_url = self._canonicalize_url(article.url or "")
            if canonical_url and canonical_url in seen_urls:
                continue

            title_norm = self._normalize_text(article.title or "")
            text_norm = self._normalize_text(self._build_similarity_text(article))
            text_tokens = self._tokenize(text_norm)

            if self._is_duplicate(title_norm, text_norm, text_tokens, kept_cache):
                continue

            kept.append(article)
            if canonical_url:
                seen_urls.add(canonical_url)
            kept_cache.append(
                {
                    "title_norm": title_norm,
                    "text_norm": text_norm,
                    "tokens": text_tokens,
                }
            )

        removed = len(articles) - len(kept)
        if removed > 0:
            logger.info(f"Deduplicated {removed} similar articles")

        return kept

    def _is_duplicate(
        self,
        title_norm: str,
        text_norm: str,
        text_tokens: list[str],
        kept_cache: list[dict[str, Any]],
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸å·²ä¿ç•™å†…å®¹è¯­ä¹‰ç›¸ä¼¼"""
        for cached in kept_cache:
            title_ratio = self._sequence_ratio(title_norm, cached["title_norm"])
            text_ratio = self._sequence_ratio(text_norm, cached["text_norm"])
            jaccard = self._jaccard_similarity(text_tokens, cached["tokens"])

            if (title_ratio >= 0.92 and text_ratio >= 0.86) or text_ratio >= 0.92 or jaccard >= 0.84:
                return True

        return False

    def _build_similarity_text(self, article: Article) -> str:
        """ç»„åˆç”¨äºç›¸ä¼¼åº¦åˆ¤æ–­çš„æ–‡æœ¬"""
        return "\n".join(
            part
            for part in [article.title, article.summary, article.core_value]
            if part
        )

    def _normalize_text(self, text: str) -> str:
        """æ¸…æ´—å¹¶è§„èŒƒåŒ–æ–‡æœ¬"""
        tokens = self._tokenize(text)
        return " ".join(tokens)

    def _tokenize(self, text: str) -> list[str]:
        """ç®€å•åˆ†è¯å¹¶å»é™¤å™ªå£°"""
        tokens = re.findall(r"[a-z0-9]+|[\u4e00-\u9fff]+", text.lower())
        cleaned: list[str] = []
        for token in tokens:
            if token.isascii():
                if len(token) > 1:
                    cleaned.append(token)
            else:
                cleaned.append(token)

        return cleaned

    def _sequence_ratio(self, a: str, b: str) -> float:
        if not a or not b:
            return 0.0
        return SequenceMatcher(None, a, b).ratio()

    def _jaccard_similarity(self, a: list[str], b: list[str]) -> float:
        if not a or not b:
            return 0.0
        a_set = set(a)
        b_set = set(b)
        if not a_set or not b_set:
            return 0.0
        return len(a_set & b_set) / len(a_set | b_set)

    def _canonicalize_url(self, url: str) -> str:
        if not url:
            return ""
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return url

            query = parse_qsl(parsed.query, keep_blank_values=True)
            filtered = [
                (k, v)
                for k, v in query
                if not k.lower().startswith("utm_")
            ]
            normalized_query = urlencode(filtered, doseq=True)
            path = parsed.path.rstrip("/")
            return urlunparse(
                (parsed.scheme, parsed.netloc, path, "", normalized_query, "")
            )
        except Exception:
            return url

    def _get_audience_size(self, article: Article) -> int:
        """ä¼°ç®—å—ä¼—è§„æ¨¡ï¼ˆstars / metadata ä¸­çš„çƒ­åº¦å­—æ®µï¼‰"""
        if article.stars:
            return int(article.stars)

        candidates = [
            "stars",
            "points",
            "score",
            "votes",
            "upvotes",
            "comments",
            "replies",
            "likes",
            "heat",
            "views",
        ]

        for key in candidates:
            value = article.metadata.get(key) if article.metadata else None
            parsed = self._parse_numeric(value)
            if parsed is not None:
                return parsed

        return 0

    def _parse_numeric(self, value: Any) -> Optional[int]:
        if value is None:
            return None

        try:
            if isinstance(value, (int, float)):
                return int(value)

            if isinstance(value, str):
                v = value.strip().lower().replace(",", "")
                if v.endswith("k"):
                    return int(float(v[:-1]) * 1000)
                if v.endswith("m"):
                    return int(float(v[:-1]) * 1000000)
                return int(float(v))
        except Exception:
            return None

        return None

    async def _generate_json(self, report: DigestReport) -> Path:
        """ç”Ÿæˆ JSON æ•°æ®"""
        filename = f"daily_report_{report.date}.json"
        filepath = self.output_dir / filename

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        data = {
            "date": report.date,
            "generated_at": report.generated_at.isoformat(),
            "stats": {
                "total_fetched": report.total_fetched,
                "total_processed": report.total_processed,
                "high_quality_count": report.high_quality_count,
                "avg_score": round(report.avg_score, 2),
                "sources": report.sources,
            },
            "articles": [
                {
                    "id": a.id,
                    "title": a.title,
                    "url": a.url,
                    "source": a.source,
                    "summary": a.summary,
                    "core_value": a.core_value,
                    "tech_stack": a.tech_stack,
                    "recommendation": a.recommendation,
                    "score": a.score,
                    "stars": a.stars,
                    "language": a.language,
                }
                for a in report.articles
            ],
            "errors": report.errors,
            "processing_time_seconds": round(report.processing_time_seconds, 2),
        }

        async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))

        return filepath

    async def generate_summary_email(
        self,
        report: DigestReport,
    ) -> str:
        """
        ç”Ÿæˆé‚®ä»¶æ‘˜è¦ï¼ˆHTML æ ¼å¼ï¼‰

        å¯ç”¨äºé‚®ä»¶é€šçŸ¥
        """
        high_quality = report.get_high_quality_articles()

        html = f"""
        <html>
        <head>
            <style>
                body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; }}
                .header {{ background: #1a1a2e; color: white; padding: 20px; }}
                .stats {{ display: flex; gap: 20px; margin: 20px 0; }}
                .stat-box {{ background: #f5f5f5; padding: 15px; border-radius: 8px; }}
                .article {{ border-bottom: 1px solid #eee; padding: 15px 0; }}
                .score {{ color: #f39c12; font-weight: bold; }}
                .tech-stack {{ color: #3498db; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ—ï¸ AI å†…å®¹è„±æ°´æ—¥æŠ¥</h1>
                <p>ğŸ“… {report.date}</p>
            </div>
            
            <div class="stats">
                <div class="stat-box">
                    <strong>{report.total_processed}</strong><br/>
                    å¤„ç†æ•°é‡
                </div>
                <div class="stat-box">
                    <strong>{report.high_quality_count}</strong><br/>
                    é«˜è´¨é‡é¡¹ç›®
                </div>
                <div class="stat-box">
                    <strong>{report.avg_score:.1f}</strong><br/>
                    å¹³å‡è¯„åˆ†
                </div>
            </div>
            
            <h2>ğŸŒŸ ä»Šæ—¥ç²¾é€‰</h2>
        """

        for i, article in enumerate(high_quality[:5], 1):
            tech_str = ", ".join(article.tech_stack) if article.tech_stack else ""
            html += f"""
            <div class="article">
                <h3>{i}. <a href="{article.url}">{article.title}</a></h3>
                <p class="score">è¯„åˆ†: {article.score}/100</p>
                <p><strong>æ ¸å¿ƒä»·å€¼:</strong> {article.core_value}</p>
                <p class="tech-stack">æŠ€æœ¯æ ˆ: {tech_str}</p>
                <p>{article.summary}</p>
            </div>
            """

        html += """
            <hr/>
            <p style="color: #888; font-size: 12px;">
                ğŸ¤– ç”± AI Daily Digest è‡ªåŠ¨ç”Ÿæˆ
            </p>
        </body>
        </html>
        """

        return html
